# Udemy Course: Machine Learning with Javascript by Stephen Grider

* [Course Link](https://www.udemy.com/machine-learning-with-javascript/)
* [Course Repo](https://github.com/StephenGrider/MLCasts)

## Section 1 - What is Machine Learning

### Lecture 2 - Solving Machine Learning Problems

* ML can do predicions based on historical data. e.g if it rain 240mm what the flood damage will there be
* The ML Problem Solving Process
	* Identify data that is relevant to the problem
	* Assemble a set of data related to the problem we're trying to solve
	* Decide on the type of output we are predicting
	* Based on type of output, pick an algorithm that will determine a correlations between our 'features' and 'labels'
	* Use model generated by algorithm to make a prediction
* For our example problem 'annual rainfall' is the feature or independent variable. Flood damage costs is the dependent variable 'label'
* in this course we will see how we can collect data format it and prepare it for ML
* we will assume we have no data. data can come from videos, newspapers, websites
* flood repair spending can come from budget reports. 
* all is assembled in a table

### Lecture 3 - A Complete Walkthrough

* Value of Labels are discrete set? Classification
* Value of Labes is continuous? Regression
* the example problem about floods is a regression problem (we predict money)
* Some takeaway points
	* Features are categories of data points that affect the value of labels
	* Datasets almost always need cleanup and formatting
	* Regression for continuous vals, Classification for diescrete
	* Many many different ML algorithms exist each with pros and cons anf or different type of problems.
	* models relate the value of features to the value of labels

### Lecture 4 - App Setup

* we go to [Github Starter Projects](https://github.com/StephenGrider/MLKits) and download them in our course workspace
* we go to '/MLKits/plinko'
* we open 'index.html'

### Lecture 5 - Problem Outline

* what we see is a game that puts (randomly?) disks in buckets increasing their counter.
* we will use it to build a dataset trying to predict in which bucket the ball will end to given the drop point

### Lecture 6 - Identifying Relevant Data

* apart from the priamry feature (drop position) we identify 2 minor features. ball size and bonciness. both are randonly selected between a configurable range of values

### Lecture 7 - Dataset Structures

* the app offers a lot of ways to record our tries and collect data (dataset)
* In JS datasets can be:
	* Arrays of objects eg `[{dropPosition: 300, bonciness: 0.4, ballSize: 16, bucket: 4}]`
	* Nested Arrays [[300,0,4,16,4],[350,0.4,35,5],[416,0.4,16,4]]. Indices are Important
* we open 'score.js'. it has 2 method signatures one to collect the data set and one to run the analyzsis to predict

### Lecture 8 - Recording Observation Data

* we flesh out 'onScoreUpdate' a callback  called everytime a round ends to collect data
```
const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition,bounciness,size,bucketLabel]);
  console.log(outputs);
}
```
* we test and see an evergroing nested array

### Lecture 9 - What Type of Problem?

* our problem is clearly a Classification
* our classification algorithm of choice is K-Nearest Neighbor (knn) is based on clustering

## Section 2 - Algorithm Overview

### Lecture 10 - How K-Nearest Neighbor Works

* Thought Experiment: what wqould happen if we dropped the ball 10 times from almost the same spot (300px)? 
* K-Nearest Neighbor (with one independent variable)
	* Drop a ball a bunch of times all around the board, record which bucket it goes to
	* For each observation subtract drop point from 300px, take absolute value
	* Sort ther results from least to greatest
	* Look at the 'k' top  records. What was the most common bucket?
	Whichever bucket came up most frequently is the one ours will probably go to

### Lecture 11 - Lodash Review

* we will use [lodash](https://lodash.com/docs/). a JS utility library with lots of methods for arrays, objects etc
* we use JS interpreter tool [JSPlaygrounds](https://stephengrider.github.io/JSPlaygrounds/) to experiment with code
* if we have a 4x2 nested array and we want to sort it by the row second element we can use stortBy selecting the element based on which we want to sort
```
const numbers = [[10,5],[17,2],[34,1],[60,-5]];
const sorted = _.sortBy(numbers, row => row[1])
```
* we map through the sorted array to extract second element `const mapped = _.map(sorted, row => row[1]);`
* we see a pattern forming where we use the previous result as input for next method .... function chaining maybe??? '_.chain' allows to chain lodash methods passing in result
```
_.chain(numbers)
	.sortBy(row=>row[1])
	.map(row=>row[1])
	.value();
```
* note that we omit the first argument as its implicitly passe don. vlaue() stops the chain adn returns the result

### Lecture 12 - Implementing KNN

* we start implementing KNN in JS using lodash in JSPlaygrounds
* we start by defining a hardcoded dataset as nested array
```
const outputs = [
	[10, .5, 16, 1].
  [200, .5, 16, 4],
  [350, .5, 16, 4],
  [600, .5, 16, 5]
];
```
* for e ach observation, we will subtract droppoint from 300px, and take the absolute value. we will use map. our result table will have to datapoints per observation. droppoint and bucket
* we put our distance calculation in a helper
```
const predictionPoint = 300;

const distance = (point) => {
	return Math.abs(point - predictionPoint);
};
```
* we implement step 1 
```

_.chain(outputs)
  .map(row => [distance(row[0]),row[3]])
```
* step 2 is to sort results from least to greatest. we use sortBy inthe chain `.sortBy(row => row[0])`
* step 3 is to look at top 'k' records. whats the most common bucket??? we use 'slice'. for k=3 `.slice(0,3)`

### Lecture 13 - Finishing KNN Implementation

* step 4 is to look in these k top records. whats the most common bucket? we will use lodash 'countBy' counting the records that meet some criteria. what we get is an object with key value pairs `.countBy(row => row[1])` => '{"1":1,"4":2}'
* next we will use lodash `.toPairs()` to convert the object to nested array
* we use sortBy to sort based on second column (occurencies). most will go to bottom
* i need t get the last element using lodash `.last()` and then `.first()` to get first element (bucket)
* then we use `.parseInt()` to turn string to bucket and `.value()` to terminate chain

### Lecture 14 - Testing the Algorithm

* we take the code and move it in the runAnalysis function in 'score.js'
* we cp distance function in the js file under analysis method, we cp globals on top and lodash chain in the runAnalysis method. we import lodash
* we test by running the game for multiple balls to fill our data set. then we 'analyze' to see in which bucket it will fall if we drop it from point 300. this will run our js knn method
* to test the result we 'reset' and drop 100 balls at 300
* we see our algorithm is way off

### Lecture 15 - Interpreting bad Results

* Steps after realising we have bad results:
	* Adjust the parameters of the analysis
	* Add more features to explain the analysis (bounciness, ball size)
	* Change the prediction point
	* Accept the fact there isn't a good correlation
* we will try out different k and rerun the analysis to see if it has effect on accuracy

### Lecture 16 - Test and Training Data

* to improve the algo we need to have a good way to compare accuracy with different settings. e.g for many different prediction points
* the way to find the ideal K
	* record a bunch of data points
	* split that data into a 'training' and a 'test' set
	* for each 'test' record run KNN using the 'training' data
	* Does the result of KNN equal the 'test' record bucket??

### Lecture 17 - Randomizing Test Data

* we implement a function to split the dataset in 2 groups
* we shuffle data to avoid bias `const shuffled = _.shuffle(data);`
* we split our dataset in 2 (train, test) using slice
```
	const testSet = _.slice(shuffled, 0, testCount);
	const trainingSet = _.slice(shuffled, testCount);
```
* we return them `return [testSet,trainingSet];`

### Lecture 18 - Generalizing KNN

* we will run 'runAnalysis()' many times for each row in testSet
* we put knn core in a helper function 'knn()' which will run on 'outputs' aka trainingSet multiple times
* in our recurcive runAnalysis each knn run will get a new predicition point which will be the dropPosition of the testSet row
* so we pass it as param passing it in the 'prediction()'
```
function knn(data, point) {
	 return _.chain(data)
	  	.map(row => [distance(row[0], point),row[3]])
		.sortBy(row => row[0])
		.slice(0,k)
		.countBy(row => row[1])
		.toPairs()
		.sortBy(row => row[1])
		.last()
		.first()
		.parseInt()
		.value();
}

function distance(pointA,pointB) {
	return Math.abs(pointA-pointB);
}
```

### Lecture 19 - Gauging Accuracy

* we assemble all pieces together in the runAnalysis function
* we split the dataset keeping 10 rows as testSet `const [testSet, trainingSet] = splitDataset(outputs, 10);`
* we do a for loop to run knn for each test datarow and we just console log result for now
```
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		console.log(bucket);
	}
```
* we test
* we need to compare knn predictions to the test set actual bucket results. we cl them  `console.log(bucket, testSet[i][3]);` accuracy is poor 

### Lecture 20 - Printing a Report

* we just increase a counter at each correct prediction and we do a cl in the end as report
```
function runAnalysis() {
	const testSetSize = 10;
	const [testSet, trainingSet] = splitDataset(outputs, testSetSize);

	let numberCorrect = 0;
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		if (bucket === testSet[i][3]){
			numberCorrect++;
		}
```

### Lecture 21 - Refactoring Accuracy Reporting

* we refactor  runAnalysis using lodash and .chain()
* we use filter to reduce the array keeping only corect predictions
```
	const accuracy _.chain(testSet)
	 .filter(testPoint => knn(trainingSet, testPoint[0]) === testPoint[3])
	 .size()
	 .divide(testSetSize)
	 .value()
```

### Lecture 22 - Investigating Optimal K Values

* we will wrap ou runAnalisis testvcode in a for loop to test the results for different K vals
* we use lodash .range() instead of for loop)
* we also pass k as parameter at knn()
* we test but we dont see a trend in results
* we change test size to 50 and then to 100 also narrow or widen the k range
* we run k up to 20 and testsize of 100 we also drop balls 1 every pixel and analyze.. we fall in accuracy

### Lecture 23 - Updating KNN for Mutiple Features

* we go to step 2 of our imporvement attempt by adding more feats to the analysis
* we ll modify the algo for multiple variables
* the only change to our algo is that we now havbe to find distance in multiple dimensions (features)
* we will add 1 more feat (bounciness) so we will work in 2d space. distance will be ((x-x0)^2 + (y-y0)^2)^0.5

### Lecture 24 - Multi-Dimensional KNN

* in a 3d space the distance would be ((x-x0)^2 + (y-y0)^2 + (z-z0)^2)^0.5

### Lecture 25 - N-Dimension Distance

* we will use all 3 feats (droppoint, bounciness, ballsize)
* we need to mod the distance method. we will make it able to work for N -dimensions, not just 3
* distance will treat pointA and B as arrays of variable length
* we use lodash and .chain()
* we use .zip() join the 2 arrays as columns
* we use .map to square the diff of 2 nums using array destructutring
* ,sum() all 
* get the value() and square it to 0.5
```
function distance(pointA,pointB) {
	return _.chain(pointA)
			.zip(pointB)
			.map(([a,b])=> (a-b)**2)
			.sum()
			.value()**0.5;
}
```

### Lecture 26 - Arbitrary Feature Spaces

* we mod knn() to be able to pas sin dimension arrays of feats
* our label is always the last element. we use lodash .initial() to get n-1 first for feats and .last() to get last element
* we dont use .pop() from vanilla JS or shift as they mod the array
* we also use .initial() at point to get rid of label so that araays in distance match in length and zip() works
* this creates a problem in the future when we will want to use the model to do predictions (we wont have a label in the pointso it will fail)
```
	 return _.chain(data)
	  	.map(row => {
	  		return [distance(_.initial(row), point), _.last(row)];
	  	})
```
* so we should not use iniital(point) but manually clear out the array in the function call `.filter(testPoint => knn(trainingSet, _.initial(testPoint), k) === testPoint[3])`

### Lecture 27 - Magnitude Offsets in Features

* we retest for 10/10 dataset. now it takes a long time
* if we plot the points in real scale we see that there is no actual variation in bounciness. so we wont get a good indication from this feat. same for ball size as distances are squared
* this is solved with normalizing feats

### Lecture 28 - Feature Normaization

* we can normalize or standardize our data
* normaization: divide by max val so that tange is 0-1
* standarization: find the standard deviation and move the 0 of our range to this point
* what i get in standarization is a normal distribution around 0 (bell curve)
* for normalization we use MinMax method: Normalized DataSet = (FeatureVal - minOfFeatureVals)/(maxOfFeatureVals - minOfFeatureVals)
* we apply normization one feat at a time
* we test it in JSPlayground using lodash
* Normalization dramaticaly improves KNN

### Lecture 29 - Normalization with MinMax

* we add a new func passing in the data nad the num of feats aka columns we want to normalize `minMax(data, featureCount)`
* label should not get normalized
* we iterate through columns we extract them with .map()
* we get minn and max of column with lodash
* we iterating thriugh column applying minMax to each element
```

function minMax(data, featureCount) {
	const clonedData = _.cloneDeep(data);

	for(let i=0;i<featureCount;i++){
		const column = clonedData.map(row=>row[i]);
		const min = _.min(column);
		const max = _.max(column);
		for(let j=0;j<column.length;j++){
			clonedData[j][i] = (clonedData[j][i] -min) / (max -min);
		}
	}

	return clonedData;
}
```

### Lecture 30 - Applying Normalization

* we test it in console and apply it in runAnalysis() in the testrtrain split `const [testSet, trainingSet] = splitDataset(minMax(outputs, 3), testSetSize);`
* we test

### Lecture 31 - Feature Selection with KNN

* our results are bad even after normalizing
* ou intution says that:
	* Changes to Drop Position: Predictable changes to Output
	* Changes to Bounciness: Changes our output, but not predictably
* In Python plotting the correlation in a scatterplot would show all these
* We test the game playing with bounciness. we see that it has a detrimental effect to the analysis. we might get better ignoring it.
* Selecting features based on the corelation with label is Feature Selection
* if we dont have tools to prove the correlation (eg Python) we can run KNN for each feature and see the results (accuracy)

### Lecture 32 - Objective Feature Picking

* we mod runAnalysis()
* we will fix k and select a column (feat)
* we limit  range(0,3) so 0 1 2 to use it for column index. we will hardcode k
* we extract feature column from data and label `const data = _.map(outputs,row=> [row[feature], _.last(row)]);`
* we move tranitestsplit in analysis passing the new dataset `const [testSet, trainingSet] = splitDataset(minMax(data, 1), testSetSize);`
* we also make knn param for label parametrical `testPoint => knn(trainingSet, _.initial(testPoint), k) === _.last(testPoint)` 

### Lecture 33 - Evaluating Different Feature Values

* we see that indeed drop position has the largest effect in KNN
* the other 2 affect the result but cannot help us predict the result

## Section 3 - Onwards to Tensorflow JS!

### Lecture 34 - Let's Get our Bearings

* Key points from our very frugal into to ML:
	* Features vs Labels
	* Test vs Train sets of data
	* Feature Normalization
	* Common data structures (nested arrays)
	* Feature Selection
* Lodash:
* Pros: 
	* methods for just about everything we need
	* Excellent API desing (e.g. .chain())
	* Skills trasferable to other JS projects
* Cons: 
	* Extremely slow (relatively)
	* Not 'numbers' focused
	* Some things are awkward (getting columns of values)
* Tesorflow JS:
* Pros:
	* Similar API to Lodash
	Extremely Fast for numeric calcuilations
	* Has a 'low-level' linear algebra API + higher level API for ML
	* Similar API to numpy (popular Python numerical lib)
* Cons: 
	* still in active development

### Lecture 35 - A Plan to Move Forward

* Plan on tackling Tensorflow JS
	* Learn some fundamentals around Tensorflow JS
	* Go through a couple of exercises with Tensorflow
	* Rebuild KNN algorithm using Tensorflow
	* Build other algorithms with Tensorflow
* Rembember that:
	* The fastest way to learn ML is to master fundamental operations around working with Data
	* Strong knowledge of data handling basics makes applying any algorithm trivial

### Lecture 36 - Tensor Shape and Dimension

* [Tensorflow JS Site](https://js.tensorflow.org/)
* Tensorflow #1 Job when you begin ML is to make working with numbers in nested arrays really easy
* The core unit in a Tensorflow program is the Tensor (A JS object that wraps a collection of numbers structured in arrays)
* A Tensor in a program language agnostic definition is a multidimensional vector
* A core property of Tensors is Dimensions (like normal arrays in any language or tables in linear algebra)
* An easy way to tell the dimensions of a Tensor is to count the opening square braces
* Linear Algebra knowledge is a MUST
* Another core property of a Tensor is Shape: How many records (elements) in each dimension AKA size of array or Table (for JS remember .length on each dimension from outside in)
* e.g [[5,10,17],[18,4,2].length=3].length=2 => Shape [2,3]
* 2D is the most important dimension we will work with. for @D shape is [#rows, #columns]
* shapes are always in brackets. even for 1d

### Lecture 37  - Elementwise Operations

* to access the tensorflow library in JSPlayground we use 'tf.'
* to create a tensor instance we use `const data = tf.tensor([1,2,3])`
* tensor comes pack with methods and properties like '.shape' `data.shape` gives [3]
* we create a second tensor `const otherData = tf.tensor([4,5,6])` 
* tensors support linear algebra mathematical operations like 
	* elementwise addition `data.add(otherData)`
	* elementwise subtraction `data.sub(otherData)` => [-3,-3,-3]
	* elementwise multiplication `data.mul(otherData)` => [4,10,18]
	* elementwise division `data.div(otherData)` => [0.25,0.4,0.5]
* elementwise operations work on elements of same index and the resuilt is in same index  in a new tensor
* elementwise operations can be comparative or logical
* if we call `data` in still [1,2,3] do elementwoise operations do not mutate operands
* for elementwise operations shapes have to match!!!
* elementwise operations work for multidiventional tensors as well

### Lecture 38 - Broadcasting Operations

* sometimes we can do operations on tensors whose shapes don't match. for elementwise operations like add the  value of the smallest shape is broadcasted to do operatons with the other elements e.g [1,2,3] + [4] = [5,6,7]
* Broadcasting works when taking the shape of  both tensors. from right to left the shapes are equal or one is 1. if they are equal normal operation takes effect. if one has shape 1  in one dimension its value is 'broadcasted to other elements' so that operation can be performed e.g [[1,2,3],[4,5,6]] + [[1],[1]] === [[1,2,3],[4,5,6]] + [[1,1,1],[1,1,1]] = [[2,3,4],[5,6,7]]

* broadcastin is alowed when there is no value in one shape. 
* so last dimension size can match or 1, previous have to match , first can match or non-exist

### Lecture 39 - Logging Tensor Data

* Tensors are JS objects so we cannot just use console.log()
* to see how they look like we use `data.print()` assuming data is a tensor object
* we cannot use `console.log(data.print())`

### Lecture 40 - Tensor Accessors 

* accessors are used for debuging purposes not for actual programs
* we make a tensor `const data = tf.tensor([10,20,30]);`
* we can access specific element giving their index e.g `data.get(0)` returns 10
* Tensors are NOT arrays. we cannot use data[0] for a multidimensional Tensor we add arguments in get(). 
* get dimensions must match get argument count
* there is no .set() method. we cannot set specific elements

### Lecture 41 - Creating Slices of Data

* We can access multiple slices of data in a tensor. no need for lodash hacking
* we add a sizable tensor
```
const data = tf.tensor([
  [10,20,30],	
  [40,50,60],	
  [10,20,30],	
  [40,50,60],	
  [10,20,30],	
  [40,50,60],		
  [10,20,30],	
  [40,50,60]
]);
```
* if we want to extract center column we use .slice() passing in the starting index and the size. 
* for our example start index is [0,1] 
* size values are not 0 indexed they are 1 based (num of elements) 
* for our example size is [8,1] : 8 rows 1 column `data.slice([0,1],[8,1])`
* if we dont want to hardcode the row count we can use `data.shape[0]` 
* data.shape is  an array
* an other way is to use -1 meaning all `data.slice([0,1],[-1,1])` or starting index to the end

### Lecture 42 - Tensor Concatenation

* to join together tensors we use .concat()
```
const tensorA = tf.tensor([
	[10,20,30],	
  [40,50,60]
]);
const tensorB = tf.tensor([
	[70,80,90],	
  [100,110,120]
]);
tensorA.concat(tensorB);
```
* the result is [[10 , 20 , 30 ], [40 , 50 , 60 ], [70 , 80 , 90 ], [100, 110, 120]]  
* concat by default works across the first dimension (row in our example) [2,3] concat [2,3] is [4,3]
* if we want to concat along a specific dimension we have to spec the dimension index get it from shape) as second argument. so to concat along columns  `tensorA.concat(tensorB,1);` results in [[10, 20, 30, 70 , 80 , 90 ], [40, 50, 60, 100, 110, 120]] or shape [2,6]
* so default is 0
* the parameter is called axis of concatenation (Sounds like Python)

### Lecture 43 - Summing Values Along an Axis

* we showcase an example. we create 2 tensors
```
const jumpData = tf.tensor([
	[70,70,70],
  [70,70,70],	
	[70,70,70],	
	[70,70,70]
]);

const playerData = tf.tensor([
	[1,170],
  [2,170],	
	[3,170],	
	[4,170]
]);
```
* first has jumps for a player . one player per row
* we want to sum the jumps and concat the result on the player data tensor
* calling `jumpData.sum()` sums all elements up. we dont want that
* to sum along an axis we use `jumpData.sum(1)` as we want to sum along the column direction
* we get [210, 210, 210, 210] so our result is transformed to 1D
* we cannot directly concat to playerData. we need to reshape

### Lecture 44 - Massaging Dimensions with ExpandDims

* we need to trasform our sum to [[210],[210],[210],[210]] so from [4] to [4,1]
* sum results in Dimension Reduction!!!!
* concat needs identical dimensions
* to Avoid Dimension Reduction and keep original dimensions in a sum we use a second argument `jumpData.sum(1,true)` 
* now i can concat along the y axis (1) `jumpData.sum(1,true).concat(playerData,1)`
* another way which is tensorflow standard of ading dimensions is  using .expandDims() 
* expandDims accepts an axis on which the expand happens
* expandDims(0) expands the dimensions of the tensor by 1 on the x axis  so [4] => [1,4]
* expandDims(1) expands the dimensions of the tensor by 1 on the y axis  so [4] => [4,1]
* for our example `jumpData.sum(1).expandDims(1).concat(playerData,1)` solves our puzzle

## Section 4 - Applications of Tensorflow

### Lecture 45 - KNN with Regression

*  Steps to Follow:
	* Apply a slightly different KNN algorithm in the browser with Tensorflow JS and fake data
	* Move KNN algorithm to our code editor with real data and run in NodeJS environment
	* Do some optimization
* OUr next example will have to do with house prices
* we will have alist of properties with their location and price
* THe main difference from previous problem in the type of problem
	* drop ball. which bucket? => discrete label => classification
	* location + feats. price of house? linear label => regression
* Our current approach of KNN Algo
	* Find distance between features and prediction point
	* Sort from lowest point to greatest
	* Take the top K records
	* Average the label value of those top K records

### Lecture 46 - A Change in Data Structure

* our data will be fake dataset of 2 feats: longitute + latitude. the label will be a house price
* in the current approach we will split the dataset in features and labels (Python SKlearn style). so we will have 2 tensors
* the rational is that we will do tensorwide operations

### Lecture 47 - KNN with Tensorflow

* we test in JSPlaygrounds
* we set a fkae data set as tensors
```
const features = tf.tensor([
	[-121, 47],
  	[-121.2, 46.5],
	[-122, 46.4],
	[-120.9, 46.7]
]);


const labels = tf.tensor([
	[200],
    [250],
	[215],
	[240]
]);
```
* index of tensors matters as it maps feats to label
* we pass in a prediction point as tensor `const predictionPoint = tf.tensor([-121,46]);`
* we will write KNN in tensorflow.... using barebones linear algebra not bult in algos
* our KNN is 2D so we have 2D distance calc
* first we find distance from pred point (in 1d) using sub() and broadcasting `features.sub(predictionPoint)`
* the we need to square the diffs (square each eleemnt) using '.pow(2)' on the tensor (we use chaining)
* then we need to sum on the y axis. so we chain `.sum(1)`
* then we need to get the root 2  we chain `.pow(0.5)`

### Lecture 48 - Maintaining Order Relationships

* our next step is to sort the results (distances) from lower to greatest.
* shuffling the feats tensor breaks our index link to the labels tensor
* also tensors cannot be sorted
* to solve indexing we concat features wit labels
* first we need to solve dimensioning issue as duming reduces dimenstios
* we fix features dimensioning making it [4,1] with `	.expandDims(1)`
* then we concat along y axis labels `.concat(labels,1)`

### Lecture 49 - Sorting Tensors

* to sort we will use tensor method 'unstack()' which splits the tensor into an array of tensors along the specified axis
* then we can sort using JS
* chaining '.unstack()' splits the tensor into an array of tensor along the x axis (rows). so 1 tensor per row.
* we can access the each row tensor giving the row index `.unstack()[i]`
* all JS arrays come with the sort() method inbuilt
* e.g. 
```
const letters = ['b','a','d','c'];
letters.sort() // ['a', 'b', 'c' 'd']
```
* JS cannot sort tensors or even objects. No error. no result
* we need to pass in sort() a callback to tell it how to sort
* the callback gets 2 arguments say (a, b) these can be any array element do the comparizon and return 1 or -1 
	* 1 means a > b
	* -1 means b > a
* e.g
```
const distances = [
	{ value: 20},
	{ value: 30},
	{ value: 5},
	{ value: 10},
];

distances.sort((a,b) => {
	return a.value > b.value ? 1 : -1;
});
```
* for our problem we can access the 1st element of each tensor with its index and .get() so `.get(0)`.
* our sorting method that we chain becomes
```
	.sort((a,b)=>{
		return a.get(0) > b.get(0) ? 1 : -1;
	})
```

### Lecture 50 - Averaging Top Values

* to take the top k records we use '.slice()'
* note that after using unstack() in our chain we work with vanilla JS arrays so we use vanilla JS array slice().
* it uses a start point and the num of elements `.slice(0,k)`
* to get the average we sum vals together and use '.reduce()' to do it
* JS arrat reduce takes in a callback with 2 args. the  accumulator and the array element. after the callback it takes the accumulators init val. it iterates through the array moding the sum at our will
* to get the average val
```
.reduce((acc,tensor)=>{
  	return acc + tensor.get(1);
	},0)/k
```
* remember the hoyse val isa t index 1
* our tensorflow based custom KNN complete
```
features
  .sub(predictionPoint)
  .pow(2)
	.sum(1)
	.pow(.5)
	.expandDims(1)
	.concat(labels,1)
	.unstack()
	.sort((a,b)=>{
		return a.get(0) > b.get(0) ? 1 : -1;
	})
	.slice(0,k)
	.reduce((acc,tensor)=>{
  	return acc + tensor.get(1);
	},0)/k
```

### Lecture 51 - Moving to the Editor

* we go to /MLKits/knn-tf folder
* our code goes to index.js
* we also have a csv with real housing data + column titles 
* we also have a 'load_csv.js' a JS file to load the csv data
* 'pacakge.json' has all the libs in. we just have to 'npm install'

### Lecture 52 - Loading CSV Data

* we start writing our index.js file
* first we import tensorflow
```
require('@tensorflow/tfjs-node');
const tf = require('@tensorflow/tfjs');
```
* thje first import tells tf how to do the calculations. using the GPU or the CPU
* our import will try to do clacs on the cpu (for gpu use `require('@tensorflow/tfjs-node-gpu');`)
* the second import is our actual import  we can use in our program
* next we import the loadCSV js file `const loadCSV = require('./load-csv');`
* we call loadCSV to read the file. we pass in the csv file name and a config object that contains:
	* a suffle option (useful in ML)
	* a splitTest setting the record count for test
	* a 'dataColumns' to import in our dataset for our analysis
	* a 'labelColumns'
* we do destructuring to get the attributes of interest out of the generated object
```
let { features, labels, testFeatures, testLabels } = loadCSV('kc_house_data.csv', {
	shuffle: true,
	splitTest: 10,
	dataColumns: ['lat','long'],
	labelColumns: ['price']
});
```
* we cl to test code correctness
* we see that the tfjs build uses only generic CPU feats not harnessing our specific CPU feats that could boost performance
* we gan compile Tesorflow if we want on our CPU to boost performance

### Lecture 53 - Running an Analysis

* we cp all knn tf code from textbook in a tnn() function
* knn's signature is `function knn(features, labels, predictionPoint, k){}`
* we want to test the method. but all our sample data are plain arrays we have to turn them to tensors first
```
features = tf.tensor(features);
labels = tf.tensor(labels);
testFeatures = tf.tensor(testFeatures);
testLabels = tf.tensor(testLabels);
```
* we call our method for a testpoint and print our prediction
```
const result = knn(features,labels,testFeatures.slice([0,0],[1,-1]),10);
console.log(`Guess: ${result} Actual: ${testLabels.get(0,0)}`);
```
* our prediction is of as we take int oaccount only the location

### Lecture 54 - Reporting Error Percentages

* we will calucalte and report the error
* error = ((expected value) - (predicted value)) / (expected value)
* `error = (testLabels.get(0,0) - result) / testLabels.get(0,0);`
* we decide to work with testdata as arrays to iterate through testset printing the eror
```
features = tf.tensor(features);
labels = tf.tensor(labels);

const result = knn(features,labels,tf.tensor(testFeatures[0]),10);
const error = (testLabels[0][0]- result) / testLabels[0][0];
console.log(`Guess: ${result} Actual: ${testLabels[0][0]}`);
console.log(`Error: ${error * 100 }%`);
```
* we loop through the whole testSet with forEach()
```
testFeatures.forEach((testPoint, index)=>{
	const result = knn(features,labels,tf.tensor(testPoint),10);
	const error = (testLabels[index][0]- result) / testLabels[index][0];
	console.log(`Guess: ${result} Actual: ${testLabels[index][0]} Error: ${error * 100 }%`);
});
```
* we are almost consantly guesing below the actual

### Lecture 55 - Normalization or Standardization

* we need to include other feats, like size
* we mod our loadCsv cofig `dataColumns: ['lat','long', 'sqft_lot'],`
* knn is dimension agnostic so we rerun test. 
* our results improve but are not optimal... we will do normalization
* in visual code we can enable excel viewer to view csv data
* surface varies much mush more than the location so it has a much larger contibution to knn
* normalization or standarization? our surface vals have a normal distribution with some edge cases... no even distribution. so is a good candidate for standarization
* standarization is not affected by edge cases that spoil our metrics

### Lecture 56 - Numerical Standarization in Tensorflow

* the formula of standarization is: (Value - Average)/StandardDeviation
* standarization is applied per column
* we use 'tf.moments()' an inbuilt method passing the tesnor to calculate it on. this method returns an object which among other contains 'mean' AKA average and 'variance'.
* variance is very closely related sto stdDev as stdDev = sqrt(variance)
* tf.moments() works dimilarly to sum. it needs an axis. other wise it works on all datapoints
* an example of doing standarization on a sample array using tf
```
const numbers = tf.tensor([
	[1,2],
	[3,4],
	[5,6]
]);
const {mean, variance } = tf.moments(numbers,0)
numbers.sub(mean).div(variance.pow(.5))
```

### Lecture 57 - Applying Standarization

* we will add standarization in knn()
* we get the mean and varianve aof all used feats in knn `const {mean, variance } = tf.moments(features,0);`
* we apply standarization in predictionPoint `const scaledPrediction = predictionPoint.sub(mean).div(variance.pow(.5))`
* also we start our knn chain by standardizing feats
* also we need to pass our prediction point as tensor....
* our results are still off

### Lecture 58 - Debugging Calculations

* we will use node debugger and chrome debugging tools
* we run `node --inspect-brk index.js` and open chrome broweser
* we navigate to 'chrome://inspect'
* we see the remote process (index.js). we click inspect and see odevtools debugger
* we place a breakpoint in knn() and run code
* we console log features (shape and print) and look oc. we look at our scaled prediction
* it looks ok (close to 0)
* we cp code in console chaining .print() and look at iteration to spot abnormal vals

### Lecture 59 - What Now?

* to improve our algo we can do other steps like check different k vals . or ad dmore feats to the analyzis
* we add 'sqft_living' to feats. imrpovement is much better

## Section 5 - Getting Started with Gradient Descent

### Lecture 60 - Linear Regression

* Linear Regression Pros:
	* Its Fast. Only train once then use it for any prediction
	* Uses methods that will be very important in more complicated ML
* Linear Regression Cons:
	* Lot harder to understant with just intuition

### Lecture 61 - Why Linear Regression?

* with linear regression we try to find an equation that relates some inidmepte variable with a dependent variable
* a good way to understand it is linear fit. finding a line that passes among the datapoints and correlates 2 vars. the better correlated the vals the best the fit
* it works for multiple independent vars (features) 
* the equvalent is a multiple dimensional plot (abstract) where the fit is not a line but a n-1 dimensional plane


### Lecture 62 - Understanding Linear Regression

* There are many methods of solving linear regression problems
	* Ordinary Least Squares
	* Generalized Least Squares
	* ...
	* Gradient Descent

* we will use Gradient Descent. It is used in Deep Learning for imrpoving the results 
* We try tfind the mathematical relation between sqft lot size (x) and price (y)
* we do a first guess of y = 0x+1. we plot it and see that the distance of the datapoints and the line is big.
* for the computer to understand how bad its guess is it uses a way to calculate error . in our case Mean Squared Error (MSE) = 1/n Σi=1->n (Guess[i] - Actual[i])^2 where n is the number of datarows or samples
* The computer repeats running MSE for a new guess tr. then compares the value with previous one
* the fact is that a nd b in our equation will be the best possible for the lowest MSE

### Lecture 63 - Guessing Coefficients with MSE

* one approach is to focus on one coefficient at a time, plotting (MSE,b) on a chart.
* it will have an U shape where the lowest point is our best guess for the coefficient b
* this is not the best approach because:
	* we dont know the possible rqnge of b
	* we dont know a step size for incrementing b
	* huge computational demands when adding in more features 
* gradient descent works with derivatives and slope. (rememeber Bolzano theorem)

### Lecture 64 - Observations Around MSE

* we know that 'b' vs MSE chart always looks likea parabolic curve 'U' shaped
* when we get bad guesses the slope is steep. as we get better tjhe slope decreases
* 'The steepness of line tells us how far we are. the'
* 'The direction of the slope tell us if optimal b less or more the currect guess'
* SO what we care is the rte of change , the slope or derivative of MSE

### Lecture 65 - Derivatives!

* the derivative of an equation gives a new equation that tells us the slope or rate at any location
* for an 1 feat 1 label plot (2d) the MSA is a power 2 function. slope is a linear equation. we need just 2 points (guesses) to plot it and find where it crosses 0 so we can find with just 2 guesses where MSE is the least

### Lecture 66 - Gradient Descent in Action

* the derivative of MSE (assuming a coefficient of y=ax+b is 0) is:
	* MSE = 1/n Σi=1->n (Guess[i] - Actual[i])^2 
	* d(MSE)/db = 2/n Σi=1->n (b - Actual[i]) 
* Gradient Descent calculation (for only 1 coefficient):
	* Pick a value of 'b'
	* Calculate the slope of MSE with b
	* Is the slope very very small? if yes terminate
	* Multiple the slope by any arbitrary small value calleda 'learning rate'
	* Subtract that from 'b'
	* REPEAT from step 2
* Smaller learning rate => slower lagortihm => better results
* GDE slows down as we approach the best MSE when it finds it it stops

### Lecture 67 - Quick Breather and Review

* we want to find an equation that relates an independent variable and a dependent variable => House Price = a * (Sqft Lot) + b
* we can guess at bvalues of 'b', then use MSE to figure out how wrong we were
* the slope, or rate of change of MSE can be used to figure out whether our 'b' value was too high or low
* take a slope of MSE at 'b' using the derivative of MSE equation
* subtract the slope from 'b' to update the guess
* REPEAT from step 3

### Lecture 68 - Why a Learning Rate

* we will answere the following questions
	* why the learning rate
	* why use derivatives? why dont compute MSE twice and compare (brute force)
	* why not set derivative equal to 0 and solve for b to find minimum MSE
* learning rate controls the rate of convergence
* we adjust b based on slope and learning rate.
* in the initial slope we care about its sign. otherwise its pure intuition. we can use any number.
* vonvergence starts fast and slows as we go to the min. magnitude of initial slope is something we dont care at all. to point it out we start with the sign of iniital slope and replace it with a arbitrary number. it converges at the same rate and at the same point givent the same learning rate
* using learning rate and not fixed steps prevents overshoot.
* optimal learning rate is problem dependent
* plotting MSE vs b as we do the convergence shows the rate. (common in Python)

### Lecture 69 -  Answering Common Questions

* we use derivatives for GDE to save compute cycles
* solving for 0 (finding the vertex point) wont work when we enter multiple coefficients (like a and b) as we will have mulitple unknowns

### Lecture 70 - Gradient Descent with Multiple Terms

* we ll revice the GD algo to be able to find a and b that give minimum MSE.
* our reviced equations are:
	* MSE = 1/n Σi=1->n ((a * x[i] + b) - Actual[i])^2 
	* b partial MSE derivative d(MSE)/db = 2/n Σi=1->n ((a * x[i] + b) - Actual[i])
	* a partial MSE derivative d(MSE)/da = 2/n Σi=1->n ( -x[i](Actual - (a * x[i] + b))
* in essence we solve the lienar equation to a and b and use it in our MSE
* THe Mulitterm GD algorthm will be:
	* Pick a value for 'b' and 'a'
	* Calculate the slope of MSE with respect to 'a' and 'b'
	* Are both slopes small enough? if so quit
	* Multiply both slopes by learning rate
	* Subtract results from 'b' and 'a'
	* Repeat from step 2

### Lecture 71 - Multiple Terms in Action

* as a start point we choose a and b = 0 calculate slopes and iterate with learing rate of 0.01.
* this lr creates a big overshoot. we decrease
* we see that our nonscaled feats create a problem in convergence
* so when doing GD always to normalization or standarization before
* in real life lr is not a broblem as we can iterate as many times as appropriate

## Section 6 - Gradient Descent with Tensorflow

### Lecture 72 - Project Overview

* we go to /MLKits/regression and run `npm install`
* the files in the directory is a csv with its loader js code
* what we want is to find the relationship between milles per gallon (dependent feature) and horsepower (independent feature)
* 'Miles Per Gallon = a * (Car Horsepower) + b'
* What we will implement.
* A JS `class LinearRegression{}` with:
	* `gradientDescent()` : run one iteration of GD and update 'a' and 'b'
	* `train()` : Run GD until we get good values for 'a' and 'b'
	* `test()` : Use a 'test' data set to evaluate the accuracy o9f our calculated 'a' and 'b' 
	* predict() : Make a prediction using our calculated 'a' and 'b'

### Lecture 73 - Data Loading

* we add a file 'index.js' for app code and a 'linear-regression.js' for the algorithm code (reusable)
* we add tensorflow to index.js and the load-csv file
```
require('@tensorflow/tfjs-node');
const tf = require('@tensorflow/tfjs');
const loadCSV = require('./load-csv');
```
* we use loadCSV to get in array form the csv file columns of interest
```
let {features, labels, testFeatures, testLabels } =   loadCSV('./cars.csv',{
	shuffle: true,
	splitTest: 50,
	dataColumns: ['horsepower'],
	labelColumns: ['mpg']
});
```

### Lecture 74 - Default ALgorithm Options

* we add the 'linear-regression.js' file
* we import tf `const tf = require('@tensorflow/tfjs');`
* we define and export an empty LR class
```
class LinearRegression {

}

module.exports = LinearRegression;
```
* we add a constructor initializing the class instance `constructor(features, labels, options)` we assume that feats and labels are tensors and options a config object
* we add default options for the Algo
* we use Object.assign to copy an object to an empty object with some default params for the algo`this.options = Object.assign({ learningRate: 0.1, iterations: 1000 }, options);`
* Object.assign() is a good trick to have in mind
* iterations refers to max iterations (if not defined otherwise)

### Lecture 75 - Formulating the Training Loop

* we add a train() method to class for our training loop
```
	train() {
		for (let i = 0; i < this.options.iterations; i++) {
			this.gradientDescent()
		}
	}
```

### Lecture 76 - Initial Gradient Descent Implementation

* we will flesh out 'gradientDescent()'
* our first GD implementation will be working but slow (plain arrays)
* after we will replace it with a much faster more lean implem but more cryptic (fancy tf code)
* in constructor we set locals  a, b to 0
* in gradientDescent() we implement the slope equations.
* a * x + b is our guess so we will first calculate it for all data rows and then us it in the equation to make them easier. we use map to iterate throught the dataset and make a new array of guesses
```
		const currentGuessesForMPG = this.features.map(row => {
			return this.a * row[0] + this.b;
		});
```

### Lecture 77 - Calculating MSE Slopes

* we start by the b MSE derivative using map and lodash
```
		const bSlope = _.sum(currentGuessesForMPG.map((guess, i) => {
			return guess - this.labels[i][0];
		}))*2/this.labels.length;
```
* in same fashion we implement MSE derivative for a 
```
		const aSlope = _.sum(currentGuessesForMPG.map((guess, i) => {
			return -1*this.features[i][0]*(this.labels[i][0] -guess);
		}))*2/this.labels.length;
```

### Lecture 78 - Updating COefficients

* we need to use slopes to update the a and b vals after multiplyinmg them with learning rate
```
		this.a -= (aSlope * this.options.learningRate);
		this.b -= (bSlope * this.options.learningRate);
```
* we save and require in index the linearRegression class `const LinearRegression = require('./linear-regression');
`

### Lecture 79 - Interpreting Results

* we create a new instanc eof the class doing one run
```const regression = new LinearRegression(features, labels, { 
	learningRate: 0.001,
	iterations: 1
});
```
* we call `regression.train();` and in train we cl the a and b `console.log(`a: ${regression.a} b: ${regression.b}`);`
* we run 100 iterations values are way way off. our learning rate is off. we are overshooting we lower lr 0.0001 values are still off.
* now is the point where we should normalize our dataset to try to improve

### Lecture 80 - Matrix Multiplication

* matrices can be expressed as tensors
* Considerations on Matrix Multiplication:
	* Are matrices eligible to be multiplied together?
	* Whats the output of matrix multiplication?
	* how we multiply matrices?
* a matrix is a tensor with 2 dimensions
* inner shapes must be the same e.g [4,2] x [2,3] = [4,3]
* we cannot swap the order in matrix multiplication . the result is not the same

### Lecture 81 - More on Matrix Multiplication

* the shape of the output is the shape of the outer shapes of the two multipliers e.g [4,2] x [2,3] = [4,3]
* matrix multipkication is done row by column summing the multiplied elements
* the first column of the resulting matrix has elements that are multiplications of all rows of 1st multiplicant with 1 column of second
* [[1,2],[3,4]] x [[1,2],[3,4]] = [[1x1+2x3,3x1+4x3],[1x2+2x4,3x2+4x4]]

### Lecture 82 - Matrix form of Slope Equations

* we will take advantage of matrix multiplication to calculate the slope with respact to a and b. 
* the new equation is: (Features * ((Features * Weights) - Labels)) where
	* Labels: Tensor of label data
	* Features: Tensor of our feature data
	* n: Number of observations
	* Weights: A and B in a tensor

### Lecture 83 - Simplification with matrix Multiplication

* to get the guess with tensors and multiplications
	* we convert our indepenetent variables (features) to tensor and multiply it with a tensor that contains the weights
	* to use matric multiplication we need to make them compatible shape as now feats is [n,1] and weights [2,1] to solve it we expnd feats adding a column of 1 to the rights so that multiplication gives ax+b1 = ax+b
* the result will be [n,1] containing the guesses

### Lecture 84 - How it All Works Together

* to cobine the 2 slope equations in one we have to find a way to combine the sumation content in botha as the rest is uniform
* in previous lecture we calculated the difference (guess)
* now we have to multiply it with the features which after expansion is a [6,2] tesnor
* guesses (weights) is [6,1] so they are not eligible for multiplication
* the trick is to transpose one matrix (feats) so it becomes [2,6]
* transposing and doing matrix multiplication of 1 row with 1 column does in essesnce also the summation as we get x1d1+x2d2+...+xndn and d1_d2+d3+..+dn. this is the summation of the 2 slopes.
* Being able to manipulate matrix multiplication is crucial for handling multi-feature ML techniques

## Section 7 - Increasing Performance with Vectorized Solutions

### Lecture 85 - Refactoring the Linear Regression Class

* We will refactor the LinearRegression class to use matrix multiplication and tensors
* Refactoring Approach
	* Refactor constructor to make 'features' and 'labels' into tensors
	* Append a column of one's to the feature tensor
	* Make a tensor for our weights as well
	* Refactor 'gradientDescent()' method to use the new equation (MatrixMult + transpose of feats)
* usually column of 1s is appended on the left . the result is the weight in their matrix are reversed [a,b]
* 1st step is straightforeward
```
		this.features = tf.tensor(features);
		this.labels = tf.tensor(labels);
```
* for 2nd step we need to generate a conlumn of 1 and concat it to the feat tensor
* genrating tables of ones or zeroes in tf is easy `tf.ones([6,1])`
* we use concat in chain mode along the y axis 
* we also have to be aware that concat return a new tensor `this.features = tf.onex([this.features.shape[0],1]).concat(this.features,1);`

### Lecture 86 - Refactoring to One Equation

* Step 3. we make a tensor out of initial weights to store our calculated weights as we move along . again we use tf `const weights = tf.zeros([2,1]);`
* Step 4. we reimplement gradient descent method
* mtarixMultiplication in tf is done with 'tf.matMul()'
* transpose in tf is easy '.transpose()' operating on tensors
* we dont care about the 2 factor in original slope equations. its just a factor. learning rate is much more crucial factor than that

### Lecture 87 - A Few More Changes

* the last part og GR method is to recalclulate the weights taking into acount slope and lr `this.weights.sub(slopes.mul(this.options.learningRate));`
* our gd method complete
```
	gradientDescent() {
		const currentGuesses = this.features.matMul(this.weights);
		const differences = currentGuesses.sub(labels);

		const slopes = this.features
			.transpose()
			.matMul(differences)
			.div(this.features.shape[0]);

		this.weights = this.weights.sub(slopes.mul(this.options.learningRate));
	}
```
* remember that tensors are Immutable. we need to assigne our operations to new or existing ones
* our approach is a Vectorized solution
* we printout our weights  in index.js `console.log(`a: ${regression.weights.get(1,0)} b: ${regression.weights.get(0,0)}`);` and test

### Lecture 88 - Same Results? or Not?

* we see that our results are much better than when we where working with arrays. omitting the 2 improves the result

### Lecture 89 - Calculating Model Accuracy

* To calculate the Model Accuracy
	* Train the Model with training data (we already do it)
	* Use 'test' data to make predictions about observations with known labels
	* Gauge accuracy by calculating coefficient of determination
* Coefficient of Determination: R^2 = 1 - (SSres/SStot)
	* SSres: (Sum of Squares of Residuals) = Σi=1->n (Actual - Predicted)^2
	* SStot: (Total Sum of Squares) = Σi=1->n (Actual - Average)^2
* R2 of 1 means perfect fit, <0 means we are way off
* Predicted val is our prediction, Average is the average or mean of all actuals
* Total Sum of Squares is the sum ofd the distance of actual values from the average or mean (strain line). is the worst case possible. we have to be neteter than this
* Sum of squares of residuals  is the sum ofd the distance of actual values from our prediction (a line made up of the calculated weights)

* negative R2 means SStot is better than SSres. our prediction is below average


### Lecture 90 - Implementing Coefficient of Determination

* we will implement a new method 'test()' that accepts test feats and labels so that it can extract accuracy metrics
* basically we do massaging of the test data like we did for train data in constructor
* we also use the weights we got from trianing to build our prediction array for the test set
```
	test(testFeatures, testLabels) {
		testFeatures = tf.tensor(testFeatures); 
		testLabels = tf.tensor(testLabels);


		testFeatures = tf
			.ones([testFeatures.shape[0],1])
			.concat(testFeatures,1);

		const predictions = testFeatures.matMul(this.weights);
```
* we add a print of our ptrediction tensors and we call it in index.js `regression.test(testFeatures, testLabels);`
* the predictions seem reasonable

### Lecture 91 - Dealing with Bad Accuracey

* we go on to calculate the R2
* calculating SSres with tf is easy
```
		const res = testLabels
			.sub(predictions)
			.pow(2)
			.sum()
			.get();
```
* sum() sums all and .get() extract the single val out of tensor
* we need to calculate the average for tot. we use .means()
* we calculate SStot
```
		const tot = testLabels
			.sub(testLabels.mean())
			.pow(2)
			.sum()
			.get();
```
* we return r2 `return 1 - (res/tot);`
* we test and r2 is off (-3)

### Lecture 92 - Reminder on Standarization

* our dataset is a standarization candidate
* we ll use tf.moments to extract mean and variance
* if we do standarization on training set we have to apply the same mean and variance on the test set so that results are relevant
* standarization will be a preprocessing feat we can apply on both in a separate method

### Lecture 93 - Data Processing in a Helper Method

* we will add processFeatures() method to be used by contructor() and test(). it will return scaled (by the same factor) and 1s appended feats ready for the processing
* we start by putting in only the existing preprocessing
```
	processFeatures(features) {
		features = tf.tensor(features);
		features = tf
			.ones([features.shape[0],1])
			.concat(features,1);

		return features;
	}
```
* we use it in both test and constructor

### Lecture 94 - Reapplying Standarization

* in preprocessing we need to calculate the mean and variance first time and then reuse them on next calls
* we need to distinguish if its going to be the first call or second
* we add a new helper to extract the metrics and store them as object attributes to be used in preprocessing. it also returns them for immediate use
```
	standardize(features) {
		const { mean, variance } = tf.moments(features,0);
		this.mean = mean;
		this.variance = variance;

		return features.sub(mean).div(variance.pow(0));
	}
```
* in preocessFeatures we decide what to do based on the mean and variance status
```
		if(this.mean && this.variance) {
			features = features.sub(this.mean).div(this.variance.pow(0));
		} else {
			features = this.standardize(features);
		}
```

### Lecture 95 - Fixing Standarization Issues

* we get worse results. we suspect standarization issues. we print our features. after standarizationm in index.js
* we see that we standardized our ones prepended column as well which is now -1....
we need to fix this
* we apply concat ones after standarization
* we fix the issue but r2 is bad
* we apply the standarization teqnique on a [10,1] tensor of ones in browes (JSPlayground) it is 1 not -0.9999 we got in node.js
* tensorflow delegates calculations to the underlying mechanism. in browser is webGL...

### Lecture 96 - MassagingLearning Rates
* our goal is to improve R2. we play with LearnignRate
* we play and wsse that 0.5 to 1 gives 0.61 so its the best we can get
* we need to start adding features...

### Lecture 97 - Moving Towards Multivariate Regression

* our new Predicition equations will be MPG = b + a1 * Feat1 + a2 * Feat2 + ... + an * Featn
* so its same just more weights
* y = b+ax (Univariate Linear Regression)
* y = b+a1x1+a2x2+...+anxn (Multivariate Linear Regression)
* Feats * Weights is same (matMul)
* Labels emain same
* transpose works the same

### Lecture 98 - Refactoring for Multivariate Analysis

* we just have to fix our weights initialization `this.weights = tf.zeros([this.features.shape[1],1]);`
* we load more columns out of csv `dataColumns: ['horsepower','weight','displacement'],` 
* we test and we are way off. we reduce lr we play with iteration
* we need to automate lr searching

### Lecture 99 - Learning Rate Optimization

* big LR can create overshoot.
* there are Learning Rate Optimization Methods available
	* Adam
	* Adagrad
	* RMSProp
	* Momentum
* they adjust the LR to find the best one
* We will build a Custom Learning Rate Optimizer
	* With every iteration of GD, calculate the exact value of MSE and store it
	* After running an iteration of GD, look at the current MSE and the old MSE
	* if the MSE iwent 'up' then we did a bad update, so divide learning rate by 2
	* if the MSE went 'down' we are in the right direction. increase LR by 5%

### Lecture 100 - Recording MSE History

* or vector based solution used the combined slope method
* we need to go back to the MSE equation
* the MSE vectorized form is MSE = sum(((Features * Weights) -Labels)^2)/n
* we impelment it in a new method pushing the mse to a local history array
```
	recordMSE() {
		const mse = this.features
			.matMul(this.weights)
			.sub(this.labels)
			.pow(2)
			.sum()
			.div(this.features.shape[0])
			.get();

		this.mseHistory.push(mse);
	}
```
* we call the method in train() after each GD iteration
* in test we dont update weights

### Lecture 101 - Updating Learning Rate

* we implement 3 last stpes of our custom algo in an updateLearningRate method
* to avoif looking for the last values to compare instead of push() we use unshift to the array to put the new vals in the beginning of the array
```
	updateLearningRate() {
		if (this.mseHistory.length < 2) {
			return;
		}

		if(this.mseHistory[0] > this.mseHistory[1]) {
			this.options.learningRate /= 2;
		} else {
			this.options.learningRate *= 1.05;
		}
	}
```
* we call updateLR after storing the mse in the train iteration

## Section 8 - Plotting Datas with Javascript

### Lecture 102 - Observing Changing Learning Rate and MSE

* we put a LR of 10. we see that with the LR custom optimizer we get convergence
* we cl  lr in each iteration
* we cl MSE and see the best we  get is 17.98

### Lecture 103 - Plotting MSE values

* we will use the 'node-remote-plot' library for plotting which we have already installed `const plot = require('node-remote-plot');`
* after training and geting the accuracy metrics we use the lib to create aplot as a png file in our folder 'plot.png'
```
plot({
	x: regression.mseHistory.reverse(),
	xLabel: 'Iteration #',
	yLabel: 'Mean Square Error'
});
```
* the method accepts a cofig object with plot params

### Lecture 104 - Plotting MSE History against B Values

* in linear regression class we add one more array called bHistory
* it will record how b changes over iterations
* we add an elelement in each training iteration `this.bHistory.push(this.weights.get(0,0));`
* we set it as xaxis in the plot
```
plot({
	x: regression.bHistory,
	y: regression.mseHistory.reverse(),
	xLabel: 'Value of B',
	yLabel: 'Mean Square Error'
});
```

## Section 9 - Gradient Descent Alterations

### Lecture 105 - Batch and Stochastic Gradient Descent

* our GD algorithm is not optimized.
* The current GD algorithm flow is:
	* Guess a starting value of B and A (A1, A2, A3 ..)
	* calculate slope of MSE using all observations in feature set and current A?B vals
	* Multiply the slope with Learning Rate
	* Update A and B
* for each iteration we use all observations (train  data)
* In Batches Gradient Descent the improvemtn will be to use a portion of the observation in each iteration (batches)
* we gain in speed
* In Stochastic Gradient Descent (SGD) we use one random observation in each iteration

### Lecture 106 - Refactoring towards BatchGradient Descent

* all code we have so far holds for both variations (matMul) only the dimensions change
* the mod for batch gradient descent is that now train() method will get the feats and labels split them in batches and feed them to gradientDescent() method for each iteration

### Lecture 107 - Determining Batch Size and Quantity

* we mod train()
* we need to define batch size. we add it in options object
```
const regression = new LinearRegression(features, labels, { 
	learningRate: .1,
	iterations: 100,
	batchSize: 10
});
```
* in ttrain() we calculate the num of batches. division is not always event. we round down `const batchQuantity = this.features.shape[0] / this.options.batchSize;`

### Lecture 108 - Iterating Over Batches

* in iterations we add a for loop for each back. in there we put gradientDescent as we want to feed data in batches. 
* we use tensor slice to split the dataset in batches
* we call GD with the batches
```
train() {
		const batchQuantity = Math.floor(
			this.features.shape[0] / this.options.batchSize
		);
		for (let i = 0; i < this.options.iterations; i++) {
			for(let j=0;j < batchQuantity; j++){
				const startIndex = j * this.options.batchSize;
				const { batchSize } = this.options;

				const featureSlice = this.features.slice(
					[startIndex, 0],
					[batchSize,-1]
				);

				const labelSlice = this.labels.slice(
					[startIndex, 0],
					[batchSize,-1]
				);

				this.gradientDescent(featureSlice,labelSlice);
			}
			this.recordMSE();
			this.updateLearningRate();
		}
	}
```

### Lecture  109 - Evaluating Batch Gradient Descent Results

* BGD is about performance (especially in big datasets)
* aloso the plot shows we converge faster
* we can decrease the iterations even 3 are enough
* to do SGD (stochastic) we set batchsize to 1 (dataset is shuffled so its random)
* R2 decreases.

### Lecture 110 - Making Predictions with the Model

* to make the model useful we need to be able to do predictions
* we add a method called predict. we want to be able to insert muptliple rows of features and get predictions for each
* we just have to calculate the guess using linear equation of weigths
* we standardize vals and do matMul with weights
```
	predict(observations){
		return this.processFeatures(observations).matMul(this.weights);
	}
```
* we do our predictions in index.js
* order of feats matter it has to match loadCSV order
```
regression.predict([
	[120,2,380]
]).print();
```

## Section 10 - Natural Binary Classification

### Lecture 111 - Introduction Logistic Regression

* Logistic Regression predicts discrete values (classification)
* the most basic form is Binary Classification ( only 2 categories in label)

### Lecture 112 - Logistic Regression in Action

* very similar to linear regression
* our problem is simple  (only 1 feat, 1 label): Given a person's age, do they prefer to read books or watch movies
* 1 feature (linear), 1 label (binary)
* the problem boils down in finding a math relationship (a formula) that relates a person's age to whether they like to read books or watch movies
* again it will look like: Preference = a * Age + b. prefernece has to be a binary value like 0 or 1
* so in classification problem swe encode (binary) or hot encode (in multiclass) the label classes into binary

### Lecture 113 - Bad Equation Fits

* linear equation with w eights gives linear vals. an approach would be to round the val to 0 or 1. vals below 0 or over 1 are bad. they have no eaning
* y=ax+b will go beyond the limits

### Lecture 114 - The Sigmoid Equation

* Sigmoid or σ() is 1/(1+e^x) gives vals between 0 and 1 in an s shape centered around z=0
* we can replace x witht he guess. y = ax+b
* if we pass the linear regression guess y = .57 * Age -11.1 into sigmod (1/(1+e^-y) it performs well and is binary

### Lecture 115 - Decision Boundaries

* sigmoid is not a step function. it has a gradual shift from 0 to 1. so we need a threshold. οor decision boundary. a common decision boundary is 0.5

### Lecture 116 - Changes for Logistic Regression

* The Logistic Regression Gradient Descent (Binary):
	* Encode label values as either 0 or 1
	* Guess a starting value of B and A (A1, A2 ...)
	* Calculate SLope of MSE using all (or batches) of observations in feature set and current A?B vals
	* Multiply slope with learning rate
	* Update A and B
	* Repeat from step 3

### Lecture 117 - Project Setup for Logistic Regression

* our problem for testing the algo has same dataset as before: Given a vehicles weight, horsepower and engine displacement will it PASS or FAIL a smog emissions check
* its a binary classification problem
* we ll refactor the project struct as we will use same loadcsv and data creating 3 subflders: /logistic-regression, /linear-regression and /data

### Lecture 119 - Importing Vehicle Data

* we implement same csv loading code like lienar regression in index.js

### Lecture 120 - Encoding Label Class Values

* the only difference from loadCSV for linear regression is that we add one more attribute to config object. 'converters' that does the converion from labels to numbers for the algorithm
```
	converters: {
		passedemissions: (value) => {
			return value === 'TRUE' ? 1 : 0;
		}
	},
```

### Lecture 121 - Updating Linear Regression for Logistic Regression

* we cp the whole code from LinearRegression class and rename in LogisticREgression
* we need to do small mods for the conversion to classification (logreg) basicallly add sigmoid and threshold
* in classification problems we use Cross Entropy instead of MSE or R2 to get a metric of how bad we guessed
* Cross Entropy: -(1/n)Σi=0->n(Actual * log(Guess) + (1-Actual) * log(1-Guess))
	* Actual: Encoded label val
	* Guess: sigmoid(ax+b)
	* n: number of observations
* Slope of Cross Entropy with Respect to A and B in vectorized form: Features.T * (sigmoid(Features * Weights) -Labels)/n
	* Labels: Tensor of our label data
	* Features: Tensor of feature data
	* n: num of observations
	* Weights: A and B in a tensor

### Lecture 122 - The Sigmoid Equation with Logistic Regression

* tensorflow has sigmoid inbuilt. '.sigmoid()' it works on a tensor and we can chain it

### Lecture 123 - A Touch More Refactoring

* in gradientDescent() we chain sigmoid after our ax+b guess `const currentGuesses = features.matMul(this.weights).sigmoid();` 
* we do the same in predict() where we do a guess
* in linear regression test() we were calculating R2 val. in classification it makes no sense
* we instantiate LogisticRegression in index.js
```
const regression = new LogisticRegression(features,labels, {
	learningRate: 0.5,
	iterations: 100,
	batchSize: 50
});

regression.train();
```
* we do a prediction as we canot test with R2.
```
regression.predict([
	[130, 307,1.75]
]).print();
```
* we get 0.23 so we need to do thresholding but is is a fail

### Lecture 124 - Gauging Classification Accuracy

* in test() we will change our accuacy metrics calculation. we will use test labels with know labels run the model get the probabilities and threshold them to get the predictions
* we will subtract the predictions from the real labels to get the fifferences. if difference is a 0 its a match if its !=0 its not a match
* we will abs() them and sum them to get a metric which will be how far we are from perfect match

### Lecture 125 - Implementing a Test Function

* the full test method
```
	test(testFeatures, testLabels) {
		const predictions = this.predict(testFeatures).round();
		testLabels = tf.tensor(testLabels);

		const incorrect = predictions.sub(testLabels).abs().sum().get();

		return (predictions.shape[0] - incorrect) /predictions.shape[0];
	}
```
* we call test in index ` console.log(`Accuracy ${regression.test(testFeatures, testLabels )*100}%`); `

### Lecture 126 -  Decision Boundaries

* we replace round() so that we can put a threshold !=0.5 
* we pass it as option to the class
* applying thresh to tf is easy '.greater(0.65)' in our case `.greater(this.options.decisionBoundary);`
* greater() returns booleans which cann ot be used for our accuracy calc
* we need to chain a `.cast('float32');` to fix that

### Lecture 127 - Mean Squared Error vs Cross Entropy

* in linear-regression we use MSE history to optimize learning rate.
* in logistic regression is not used but we still use it for learnign rate.
* it is useful as MSE is still used for the guess before the sigmoid. so it still has effect on  prediction. MSE is not moving  on parabola due to the sigmoid but it has a waved shape with global minimum.
* so we need to mod our learning rate optimizer to use cross entropy

### Lecture 128 - Refactoring with Cross Entropy

* we refactor recordMSE()  to use cross entropy instead
* we use vectorized way of calculating Cross Entropy (Cost): -(1/m) * (Actual^T * log(Guesses) + (1 - Actual)^T * log(1-Guesses))  ^T means matrix transpose
* in tf we transpoise with `.transpose()` we calc log with `.log()`
* we can do nested tf operations
```
	recordCost() {
		const guesses = this.features
			.matMul(this.weights)
			.sigmoid();

		const termOne = this.labels
			.transpose()
			matMul(guesses.log());

		const termTwo = this.labels
			.mul(-1)
			.add(1)
			.transpose(
				guesses
					.mul(-1)
					.add(1)
					.log()
			);

		const cost = termOne.add(termTwo)
			.divide(this.features.shape[0])
			.mul(-1)
			.get(0,0);

		this.costHistory.unshift(cost);
	}
```

### Lecture 130 - Plotting Changing Cost history

* we import plot in index.js and add aplot
```
plot({
	x: regression.costHistory.reverse()
});
```
* we play with batch sze to see conversion

## Section 11 - Multi-Value Classification

### Lecture 131 - Multinominal Logistic Regression

* Multinominal = Multiple Classification Options = Multiple Classes in label Column

### Lecture 132 - A Smart Refactor to Multinominal Analysis

* A simple problem to showcase the Smart Refactor: given a persons age do they prefer movies, books or dance
* The Smart Way is Hot Encoding. 1 column  per LABEL CLASS (BINARY aka true or false)
* the result is a probabilty per label (lice sigmoid before thresholiding)

### Lecture 133 - A Smarter Refactor

* we start with Hot Encoding of labels
* then we do logistic regression for each hot encoded label class
* features is same. weights and labels tensors only difer for each endoced class regression
* manage a separate instance for each value is a pain. can we just use one class instance?

### Lecture 134 - A Single Instance Approach

* add a dimension? put them in an array?
* the best way is to concat weights and hot encoded label tensors in one tensor addin one dimension. (y axis)
* our codebase needs refactoring but matMul works in any dimension (we mod 3nd multiplicant external dimension from 1 to say n) so matMul works

### Lecture 135 - Refactoring to Multi-Column Weights

* we add a new folder. a cp of logistic regression naming it multinominal-logistic-regressionvp 
* we mod the weights to add one more dim
* hot encoding is done in preprocessing (loadCSV so we assum labels is a [n,3] tensor) `this.weights = tf.zeros([this.features.shape[1], this.labels.shape[1]]);`

### Lecture 136 - A Problem to Test Multinominal Classification

* the problem will be. given the horsepower, weight and displacement of a vehicle. will it have high medium or low fuel efficiency?
* we dont have such a able column. we will make it by applying thresholding to mpg column to produce these 3 classes before we run LG. the rule we will apply is
	* 0-15mpg => Low
	* 15-30mpg=> Medium
	* 30+ mpg => High

### Lecture 137 - Classifying Continuous Values

* in index.js we will do the preprocessing  and hotencoding in the converters attribute of the loadCSV config obj
```

let {features, labels, testFeatures, testLabels } =   loadCSV('../data/cars.csv',{
	shuffle: true,
	splitTest: 50,
	converters: {
		mpg: (value) => {
			const mpg = parseFloat(value);
			if(mpg <15) {
				return[1,0,0];
			} else if (mpg < 30) {
				return [0,1,0];
			} else {
				return [0,0,1];
			}
		}
	},
	dataColumns: ['horsepower','displacement','weight'],
	labelColumns: ['mpg']
});
```

### Lecture 138 - Training a Multinominal Model

* we need to add lodash .flatMap() on labels to remove one level of brackets
* we check weiths tensor and its ok
* we make a prediction totest the algo for multinominal `regression.predict([[215,440,2.16]]).print();` and get [1,0,0] which is low and its correct

### Lecture 139 - Marginal vs Conditional Probability

* to do multinominal thresholding ad get  aprediction we need to choose the element  with the highest probabilty among predicted labels
* we run a prediction that returns [1,1,0] as both low and med get >0.5 probabiltiy
* sigmoid gives the probailtiy of being 1 label and in our case its done for each label class column in isolation. this is a Marginal Prop Distribution
* Marginal Probability Distribution: Considers one possible cas in isolation
* Conditional Probability Distribution: Considers all possible output cases together
* the result of Marginal Prop Distribution is to get a total probability of > 1
* Contitional Prop Distr gives a total of 1 prob in multinominal class propblems

### Lecture 140 - Sigmoid Vs Softmax

* In a nutshell: 
	* Sigmoid=> Marginal Prop Distr => Binary Clasification
	* Softmax=> Conditional Prop Distr => Multinominal Clasification
* Softmax Equation: Probability of being the 1 label rather than the 0 label
	* (e^(ax+b))/Σk=0->K(e^(ax+b))

### Lecture 141 - Refactoring Sigmoid to Softmax

* we replace sigmoid() with softmax()

### Lecture 142 - Implementing Accuracy Gauges

* to apply a trheshold for the array of labels we use argMax returns the column index with highest probability. we will apply it to equal and predictions to get the difference and have an metric

### Lecture 143 - Calculating Accuracy

* decision boundary and greater() is useless . we replace it with `.argax(1)` specing the axis it will operate (y)
* we add it in predict and test
```
  predict(observations) {
    return this.processFeatures(observations)
      .matMul(this.weights)
      .softmax()
      .argMax(1);
  }

  test(testFeatures, testLabels) {
    const predictions = this.predict(testFeatures);
    testLabels = tf.tensor(testLabels).argMax(1);

    const incorrect = predictions
      .notEqual(testLabels)
      .sum()
      .get();

    return (predictions.shape[0] - incorrect) / predictions.shape[0];
  }
```

## Section 12 - Image Recognition In Action

### Lecture 144 - Handwriting Recognition

* We will see the famous K-MNIST dataset problem : Given a pixel intensity in an image, identify whether the character is a handwritten 0,1,2,3,4,5,6,7,8 or 9
* KMNIST has 60k training images and 20k test images
* its a classical multinominal classsification problem

### Lecture 145 - Grayscale Values

* he wants to apply logistic regression to solve it... WTF????
* image is a 2d array of a val (for grayscale) that 0->255 or 0. to 1.
* MNIST is 28x28

### Lecture 146 - Many Features

* a solution to apply LR is to flattent the image into a 1d array of 784 feat columns considering a pixel ad datapoint.

### Lecture 147 - Flattening Image Data

* we have a npm package called 'mnist-data' to conveniently load the mnist dataset
* in /multinominal-logistic-regression folder  in index.js we import it `const mnist = require('mnist-data');`
* mnist contains a lot of methods. we use .training passing the range to get the 1st training image `const mnistData = mnist.training(0,1);`
* we cl it and get
```
{ images: 
   { magic_number: 2051,
     total_num_items: 60000,
     rows: 28,
     cols: 28,
     values: [ [Array] ],
     start: 0,
     end: 1 },
  labels: 
   { magic_number: 2049,
     total_num_items: 60000,
     values: [ 5 ],
     start: 0,
     end: 1 } }

```
* its an object, in images.values prop we have a nested array with the pixel values of the retrieved images. we also get in labels.values an array witht he labels
* vals are 0-255
* we want to flaten it out so we use lodash flatMap and vanilla JS map to iterate thought the top level array `const features = mnistData.image.values.map(image => _.flatMap(image))`
* we loaad 10 images and get featrures its a 2d array

### Lecture 148 - Encoding Label Values

* we need to get lables from mnist and hot encode them mnist label vals give us the index in the label array
```
const encodedLabels = mnistData.labels.values.map(label => {
	const row = new Array(10).fill(0);
	row[label] = 1
	return row;
});
```

### Lecture 149 - Implementing an Accuracy Gauge

* we create an algo instance and train it
```
const regression = new LogisticRegression(features,encodedLabels,{
	learningRate: 1,
	iterations: 5,
	batchSize: 100
});
regression.train();
```
* we get some testing data from the mnist using .testing() method `const testMnistData = mnist.testing(0,100);` its an object with same struct like trainind data
* so we do the same to extract the dat asin a meaningful way
```
const testMnistData = mnist.testing(0,100);
const testFeatures = testMnistData.images.values.map(image => _.flatMap(image));
const testEncodedLabels = testMnistData.labels.values.map(label => {
	const row = new Array(10).fill(0);
	row[label] = 1
	return row;
});

const accuracy = regression.test(testFeatures, testEncodedLabels);
console.log('Accuracy is',accuracy);
```
* our results are awful

### Lecture 150 - Unchanging Accuracy

* whatever params we tweak. accuracy is 8% fixed

### Lecture 151 - Debugging the Calculation Process

* we run our debugger with `node --inspect-brk index.js` and go to chrome://inspect
* we add a breakpoint to train() call go down to gradientDescent and cl all my vars
* when we cl this.features we see NaNs
* something is off with standarization probably as flattening is ok
* we rerun and put debugger breakpoint in processFeatrures()
* with debuggin we find that in the standarization process 'standardize()' the issue is produced by `return features.sub(mean).div(variance.pow(0.5));` where it does division with 0 returning NaN

### Lecture 152 - Dealing with Zero Variances

* We got 2 possible solutions to our situation
	* Remove the columns with only zeros from our feature set - the dont provide any benefit
	* Change our method of standarization/normalization to better acount for possible all zero vals
* We chose the 2nd option. 1st option is difficult to implement with tf
* note that in browser the code runs because tf uses webGL that deals with such issues
* what we will do is replace variance of zero with 1
* the trick to do it is `variance.add(variacne.cast('bool').logicalNot().cast('float32'))` is like xoring and masking. adding a 1 only if variance is 0

### Lecture 153 - Backfilling Variance

* we fix standardize function
```
  standardize(features) {
    const { mean, variance } = tf.moments(features, 0);

    const filler = variance.cast('bool').logicalNot().cast('float32');

    this.mean = mean;
    this.variance = variance.add(filler);

    return features.sub(mean).div(this.variance.pow(0.5));
  }
```
* our accuracy is 0.88

## Section 13 - Performance Optimization

### Lecture 154 - Handing Large Datasets

* we run the analysis for 60000 samples and it crashes. node runs out of memory
* node heap memory gets 1.8GB of Mem
* an easy fix is to allocate more meory to the task `node --max-old-space-size=4096 index.js` and allocate  4GB to the task
* we gat some warnings.. but it finishes with 0.91 accuracy

### Lecture 155 - Minimizing Memory Usage

* 