# Udemy Course: Machine Learning with Javascript by Stephen Grider

* [Course Link](https://www.udemy.com/machine-learning-with-javascript/)
* [Course Repo](https://github.com/StephenGrider/MLCasts)

## Section 1 - What is Machine Learning

### Lecture 2 - Solving Machine Learning Problems

* ML can do predicions based on historical data. e.g if it rain 240mm what the flood damage will there be
* The ML Problem Solving Process
	* Identify data that is relevant to the problem
	* Assemble a set of data related to the problem we're trying to solve
	* Decide on the type of output we are predicting
	* Based on type of output, pick an algorithm that will determine a correlations between our 'features' and 'labels'
	* Use model generated by algorithm to make a prediction
* For our example problem 'annual rainfall' is the feature or independent variable. Flood damage costs is the dependent variable 'label'
* in this course we will see how we can collect data format it and prepare it for ML
* we will assume we have no data. data can come from videos, newspapers, websites
* flood repair spending can come from budget reports. 
* all is assembled in a table

### Lecture 3 - A Complete Walkthrough

* Value of Labels are discrete set? Classification
* Value of Labes is continuous? Regression
* the example problem about floods is a regression problem (we predict money)
* Some takeaway points
	* Features are categories of data points that affect the value of labels
	* Datasets almost always need cleanup and formatting
	* Regression for continuous vals, Classification for diescrete
	* Many many different ML algorithms exist each with pros and cons anf or different type of problems.
	* models relate the value of features to the value of labels

### Lecture 4 - App Setup

* we go to [Github Starter Projects](https://github.com/StephenGrider/MLKits) and download them in our course workspace
* we go to '/MLKits/plinko'
* we open 'index.html'

### Lecture 5 - Problem Outline

* what we see is a game that puts (randomly?) disks in buckets increasing their counter.
* we will use it to build a dataset trying to predict in which bucket the ball will end to given the drop point

### Lecture 6 - Identifying Relevant Data

* apart from the priamry feature (drop position) we identify 2 minor features. ball size and bonciness. both are randonly selected between a configurable range of values

### Lecture 7 - Dataset Structures

* the app offers a lot of ways to record our tries and collect data (dataset)
* In JS datasets can be:
	* Arrays of objects eg `[{dropPosition: 300, bonciness: 0.4, ballSize: 16, bucket: 4}]`
	* Nested Arrays [[300,0,4,16,4],[350,0.4,35,5],[416,0.4,16,4]]. Indices are Important
* we open 'score.js'. it has 2 method signatures one to collect the data set and one to run the analyzsis to predict

### Lecture 8 - Recording Observation Data

* we flesh out 'onScoreUpdate' a callback  called everytime a round ends to collect data
```
const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition,bounciness,size,bucketLabel]);
  console.log(outputs);
}
```
* we test and see an evergroing nested array

### Lecture 9 - What Type of Problem?

* our problem is clearly a Classification
* our classification algorithm of choice is K-Nearest Neighbor (knn) is based on clustering

## Section 2 - Algorithm Overview

### Lecture 10 - How K-Nearest Neighbor Works

* Thought Experiment: what wqould happen if we dropped the ball 10 times from almost the same spot (300px)? 
* K-Nearest Neighbor (with one independent variable)
	* Drop a ball a bunch of times all around the board, record which bucket it goes to
	* For each observation subtract drop point from 300px, take absolute value
	* Sort ther results from least to greatest
	* Look at the 'k' top  records. What was the most common bucket?
	Whichever bucket came up most frequently is the one ours will probably go to

### Lecture 11 - Lodash Review

* we will use [lodash](https://lodash.com/docs/). a JS utility library with lots of methods for arrays, objects etc
* we use JS interpreter tool [JSPlaygrounds](https://stephengrider.github.io/JSPlaygrounds/) to experiment with code
* if we have a 4x2 nested array and we want to sort it by the row second element we can use stortBy selecting the element based on which we want to sort
```
const numbers = [[10,5],[17,2],[34,1],[60,-5]];
const sorted = _.sortBy(numbers, row => row[1])
```
* we map through the sorted array to extract second element `const mapped = _.map(sorted, row => row[1]);`
* we see a pattern forming where we use the previous result as input for next method .... function chaining maybe??? '_.chain' allows to chain lodash methods passing in result
```
_.chain(numbers)
	.sortBy(row=>row[1])
	.map(row=>row[1])
	.value();
```
* note that we omit the first argument as its implicitly passe don. vlaue() stops the chain adn returns the result

### Lecture 12 - Implementing KNN

* we start implementing KNN in JS using lodash in JSPlaygrounds
* we start by defining a hardcoded dataset as nested array
```
const outputs = [
	[10, .5, 16, 1].
  [200, .5, 16, 4],
  [350, .5, 16, 4],
  [600, .5, 16, 5]
];
```
* for e ach observation, we will subtract droppoint from 300px, and take the absolute value. we will use map. our result table will have to datapoints per observation. droppoint and bucket
* we put our distance calculation in a helper
```
const predictionPoint = 300;

const distance = (point) => {
	return Math.abs(point - predictionPoint);
};
```
* we implement step 1 
```

_.chain(outputs)
  .map(row => [distance(row[0]),row[3]])
```
* step 2 is to sort results from least to greatest. we use sortBy inthe chain `.sortBy(row => row[0])`
* step 3 is to look at top 'k' records. whats the most common bucket??? we use 'slice'. for k=3 `.slice(0,3)`

### Lecture 13 - Finishing KNN Implementation

* step 4 is to look in these k top records. whats the most common bucket? we will use lodash 'countBy' counting the records that meet some criteria. what we get is an object with key value pairs `.countBy(row => row[1])` => '{"1":1,"4":2}'
* next we will use lodash `.toPairs()` to convert the object to nested array
* we use sortBy to sort based on second column (occurencies). most will go to bottom
* i need t get the last element using lodash `.last()` and then `.first()` to get first element (bucket)
* then we use `.parseInt()` to turn string to bucket and `.value()` to terminate chain

### Lecture 14 - Testing the Algorithm

* we take the code and move it in the runAnalysis function in 'score.js'
* we cp distance function in the js file under analysis method, we cp globals on top and lodash chain in the runAnalysis method. we import lodash
* we test by running the game for multiple balls to fill our data set. then we 'analyze' to see in which bucket it will fall if we drop it from point 300. this will run our js knn method
* to test the result we 'reset' and drop 100 balls at 300
* we see our algorithm is way off

### Lecture 15 - Interpreting bad Results

* Steps after realising we have bad results:
	* Adjust the parameters of the analysis
	* Add more features to explain the analysis (bounciness, ball size)
	* Change the prediction point
	* Accept the fact there isn't a good correlation
* we will try out different k and rerun the analysis to see if it has effect on accuracy

### Lecture 16 - Test and Training Data

* to improve the algo we need to have a good way to compare accuracy with different settings. e.g for many different prediction points
* the way to find the ideal K
	* record a bunch of data points
	* split that data into a 'training' and a 'test' set
	* for each 'test' record run KNN using the 'training' data
	* Does the result of KNN equal the 'test' record bucket??

### Lecture 17 - Randomizing Test Data

* we implement a function to split the dataset in 2 groups
* we shuffle data to avoid bias `const shuffled = _.shuffle(data);`
* we split our dataset in 2 (train, test) using slice
```
	const testSet = _.slice(shuffled, 0, testCount);
	const trainingSet = _.slice(shuffled, testCount);
```
* we return them `return [testSet,trainingSet];`

### Lecture 18 - Generalizing KNN

* we will run 'runAnalysis()' many times for each row in testSet
* we put knn core in a helper function 'knn()' which will run on 'outputs' aka trainingSet multiple times
* in our recurcive runAnalysis each knn run will get a new predicition point which will be the dropPosition of the testSet row
* so we pass it as param passing it in the 'prediction()'
```
function knn(data, point) {
	 return _.chain(data)
	  	.map(row => [distance(row[0], point),row[3]])
		.sortBy(row => row[0])
		.slice(0,k)
		.countBy(row => row[1])
		.toPairs()
		.sortBy(row => row[1])
		.last()
		.first()
		.parseInt()
		.value();
}

function distance(pointA,pointB) {
	return Math.abs(pointA-pointB);
}
```

### Lecture 19 - Gauging Accuracy

* we assemble all pieces together in the runAnalysis function
* we split the dataset keeping 10 rows as testSet `const [testSet, trainingSet] = splitDataset(outputs, 10);`
* we do a for loop to run knn for each test datarow and we just console log result for now
```
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		console.log(bucket);
	}
```
* we test
* we need to compare knn predictions to the test set actual bucket results. we cl them  `console.log(bucket, testSet[i][3]);` accuracy is poor 

### Lecture 20 - Printing a Report

* we just increase a counter at each correct prediction and we do a cl in the end as report
```
function runAnalysis() {
	const testSetSize = 10;
	const [testSet, trainingSet] = splitDataset(outputs, testSetSize);

	let numberCorrect = 0;
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		if (bucket === testSet[i][3]){
			numberCorrect++;
		}
```

### Lecture 21 - Refactoring Accuracy Reporting

* we refactor  runAnalysis using lodash and .chain()
* we use filter to reduce the array keeping only corect predictions
```
	const accuracy _.chain(testSet)
	 .filter(testPoint => knn(trainingSet, testPoint[0]) === testPoint[3])
	 .size()
	 .divide(testSetSize)
	 .value()
```

### Lecture 22 - Investigating Optimal K Values

* we will wrap ou runAnalisis testvcode in a for loop to test the results for different K vals
* we use lodash .range() instead of for loop)
* we also pass k as parameter at knn()
* we test but we dont see a trend in results
* we change test size to 50 and then to 100 also narrow or widen the k range
* we run k up to 20 and testsize of 100 we also drop balls 1 every pixel and analyze.. we fall in accuracy

### Lecture 23 - Updating KNN for Mutiple Features

* we go to step 2 of our imporvement attempt by adding more feats to the analysis
* we ll modify the algo for multiple variables
* the only change to our algo is that we now havbe to find distance in multiple dimensions (features)
* we will add 1 more feat (bounciness) so we will work in 2d space. distance will be ((x-x0)^2 + (y-y0)^2)^0.5

### Lecture 24 - Multi-Dimensional KNN

* in a 3d space the distance would be ((x-x0)^2 + (y-y0)^2 + (z-z0)^2)^0.5

### Lecture 25 - N-Dimension Distance

* we will use all 3 feats (droppoint, bounciness, ballsize)
* we need to mod the distance method. we will make it able to work for N -dimensions, not just 3
* distance will treat pointA and B as arrays of variable length
* we use lodash and .chain()
* we use .zip() join the 2 arrays as columns
* we use .map to square the diff of 2 nums using array destructutring
* ,sum() all 
* get the value() and square it to 0.5
```
function distance(pointA,pointB) {
	return _.chain(pointA)
			.zip(pointB)
			.map(([a,b])=> (a-b)**2)
			.sum()
			.value()**0.5;
}
```

### Lecture 26 - Arbitrary Feature Spaces

* we mod knn() to be able to pas sin dimension arrays of feats
* our label is always the last element. we use lodash .initial() to get n-1 first for feats and .last() to get last element
* we dont use .pop() from vanilla JS or shift as they mod the array
* we also use .initial() at point to get rid of label so that araays in distance match in length and zip() works
* this creates a problem in the future when we will want to use the model to do predictions (we wont have a label in the pointso it will fail)
```
	 return _.chain(data)
	  	.map(row => {
	  		return [distance(_.initial(row), point), _.last(row)];
	  	})
```
* so we should not use iniital(point) but manually clear out the array in the function call `.filter(testPoint => knn(trainingSet, _.initial(testPoint), k) === testPoint[3])`

### Lecture 27 - Magnitude Offsets in Features

* we retest for 10/10 dataset. now it takes a long time
* if we plot the points in real scale we see that there is no actual variation in bounciness. so we wont get a good indication from this feat. same for ball size as distances are squared
* this is solved with normalizing feats

### Lecture 28 - Feature Normaization

* we can normalize or standardize our data
* normaization: divide by max val so that tange is 0-1
* standarization: find the standard deviation and move the 0 of our range to this point
* what i get in standarization is a normal distribution around 0 (bell curve)
* for normalization we use MinMax method: Normalized DataSet = (FeatureVal - minOfFeatureVals)/(maxOfFeatureVals - minOfFeatureVals)
* we apply normization one feat at a time
* we test it in JSPlayground using lodash
* Normalization dramaticaly improves KNN

### Lecture 29 - Normalization with MinMax

* we add a new func passing in the data nad the num of feats aka columns we want to normalize `minMax(data, featureCount)`
* label should not get normalized
* we iterate through columns we extract them with .map()
* we get minn and max of column with lodash
* we iterating thriugh column applying minMax to each element
```

function minMax(data, featureCount) {
	const clonedData = _.cloneDeep(data);

	for(let i=0;i<featureCount;i++){
		const column = clonedData.map(row=>row[i]);
		const min = _.min(column);
		const max = _.max(column);
		for(let j=0;j<column.length;j++){
			clonedData[j][i] = (clonedData[j][i] -min) / (max -min);
		}
	}

	return clonedData;
}
```

### Lecture 30 - Applying Normalization

* we test it in console and apply it in runAnalysis() in the testrtrain split `const [testSet, trainingSet] = splitDataset(minMax(outputs, 3), testSetSize);`
* we test

### Lecture 31 - Feature Selection with KNN

* our results are bad even after normalizing
* ou intution says that:
	* Changes to Drop Position: Predictable changes to Output
	* Changes to Bounciness: Changes our output, but not predictably
* In Python plotting the correlation in a scatterplot would show all these
* We test the game playing with bounciness. we see that it has a detrimental effect to the analysis. we might get better ignoring it.
* Selecting features based on the corelation with label is Feature Selection
* if we dont have tools to prove the correlation (eg Python) we can run KNN for each feature and see the results (accuracy)

### Lecture 32 - Objective Feature Picking

* we mod runAnalysis()
* we will fix k and select a column (feat)
* we limit  range(0,3) so 0 1 2 to use it for column index. we will hardcode k
* we extract feature column from data and label `const data = _.map(outputs,row=> [row[feature], _.last(row)]);`
* we move tranitestsplit in analysis passing the new dataset `const [testSet, trainingSet] = splitDataset(minMax(data, 1), testSetSize);`
* we also make knn param for label parametrical `testPoint => knn(trainingSet, _.initial(testPoint), k) === _.last(testPoint)` 

### Lecture 33 - Evaluating Different Feature Values

* we see that indeed drop position has the largest effect in KNN
* the other 2 affect the result but cannot help us predict the result

## Section 3 - Onwards to Tensorflow JS!

### Lecture 34 - Let's Get our Bearings

* Key points from our very frugal into to ML:
	* Features vs Labels
	* Test vs Train sets of data
	* Feature Normalization
	* Common data structures (nested arrays)
	* Feature Selection
* Lodash:
* Pros: 
	* methods for just about everything we need
	* Excellent API desing (e.g. .chain())
	* Skills trasferable to other JS projects
* Cons: 
	* Extremely slow (relatively)
	* Not 'numbers' focused
	* Some things are awkward (getting columns of values)
* Tesorflow JS:
* Pros:
	* Similar API to Lodash
	Extremely Fast for numeric calcuilations
	* Has a 'low-level' linear algebra API + higher level API for ML
	* Similar API to numpy (popular Python numerical lib)
* Cons: 
	* still in active development

### Lecture 35 - A Plan to Move Forward

* Plan on tackling Tensorflow JS
	* Learn some fundamentals around Tensorflow JS
	* Go through a couple of exercises with Tensorflow
	* Rebuild KNN algorithm using Tensorflow
	* Build other algorithms with Tensorflow
* Rembember that:
	* The fastest way to learn ML is to master fundamental operations around working with Data
	* Strong knowledge of data handling basics makes applying any algorithm trivial

### Lecture 36 - Tensor Shape and Dimension

* [Tensorflow JS Site](https://js.tensorflow.org/)
* Tensorflow #1 Job when you begin ML is to make working with numbers in nested arrays really easy
* The core unit in a Tensorflow program is the Tensor (A JS object that wraps a collection of numbers structured in arrays)
* A Tensor in a program language agnostic definition is a multidimensional vector
* A core property of Tensors is Dimensions (like normal arrays in any language or tables in linear algebra)
* An easy way to tell the dimensions of a Tensor is to count the opening square braces
* Linear Algebra knowledge is a MUST
* Another core property of a Tensor is Shape: How many records (elements) in each dimension AKA size of array or Table (for JS remember .length on each dimension from outside in)
* e.g [[5,10,17],[18,4,2].length=3].length=2 => Shape [2,3]
* 2D is the most important dimension we will work with. for @D shape is [#rows, #columns]
* shapes are always in brackets. even for 1d

### Lecture 37  - Elementwise Operations

* to access the tensorflow library in JSPlayground we use 'tf.'
* to create a tensor instance we use `const data = tf.tensor([1,2,3])`
* tensor comes pack with methods and properties like '.shape' `data.shape` gives [3]
* we create a second tensor `const otherData = tf.tensor([4,5,6])` 
* tensors support linear algebra mathematical operations like 
	* elementwise addition `data.add(otherData)`
	* elementwise subtraction `data.sub(otherData)` => [-3,-3,-3]
	* elementwise multiplication `data.mul(otherData)` => [4,10,18]
	* elementwise division `data.div(otherData)` => [0.25,0.4,0.5]
* elementwise operations work on elements of same index and the resuilt is in same index  in a new tensor
* elementwise operations can be comparative or logical
* if we call `data` in still [1,2,3] do elementwoise operations do not mutate operands
* for elementwise operations shapes have to match!!!
* elementwise operations work for multidiventional tensors as well

### Lecture 38 - Broadcasting Operations

* sometimes we can do operations on tensors whose shapes don't match. for elementwise operations like add the  value of the smallest shape is broadcasted to do operatons with the other elements e.g [1,2,3] + [4] = [5,6,7]
* Broadcasting works when taking the shape of  both tensors. from right to left the shapes are equal or one is 1. if they are equal normal operation takes effect. if one has shape 1  in one dimension its value is 'broadcasted to other elements' so that operation can be performed e.g [[1,2,3],[4,5,6]] + [[1],[1]] === [[1,2,3],[4,5,6]] + [[1,1,1],[1,1,1]] = [[2,3,4],[5,6,7]]

* broadcastin is alowed when there is no value in one shape. 
* so last dimension size can match or 1, previous have to match , first can match or non-exist

### Lecture 39 - Logging Tensor Data

* Tensors are JS objects so we cannot just use console.log()
* to see how they look like we use `data.print()` assuming data is a tensor object
* we cannot use `console.log(data.print())`

### Lecture 40 - Tensor Accessors 

* accessors are used for debuging purposes not for actual programs
* we make a tensor `const data = tf.tensor([10,20,30]);`
* we can access specific element giving their index e.g `data.get(0)` returns 10
* Tensors are NOT arrays. we cannot use data[0] for a multidimensional Tensor we add arguments in get(). 
* get dimensions must match get argument count
* there is no .set() method. we cannot set specific elements

### Lecture 41 - Creating Slices of Data

* We can access multiple slices of data in a tensor. no need for lodash hacking
* we add a sizable tensor
```
const data = tf.tensor([
  [10,20,30],	
  [40,50,60],	
  [10,20,30],	
  [40,50,60],	
  [10,20,30],	
  [40,50,60],		
  [10,20,30],	
  [40,50,60]
]);
```
* if we want to extract center column we use .slice() passing in the starting index and the size. 
* for our example start index is [0,1] 
* size values are not 0 indexed they are 1 based (num of elements) 
* for our example size is [8,1] : 8 rows 1 column `data.slice([0,1],[8,1])`
* if we dont want to hardcode the row count we can use `data.shape[0]` 
* data.shape is  an array
* an other way is to use -1 meaning all `data.slice([0,1],[-1,1])` or starting index to the end

### Lecture 42 - Tensor Concatenation

* to join together tensors we use .concat()
```
const tensorA = tf.tensor([
	[10,20,30],	
  [40,50,60]
]);
const tensorB = tf.tensor([
	[70,80,90],	
  [100,110,120]
]);
tensorA.concat(tensorB);
```
* the result is [[10 , 20 , 30 ], [40 , 50 , 60 ], [70 , 80 , 90 ], [100, 110, 120]]  
* concat by default works across the first dimension (row in our example) [2,3] concat [2,3] is [4,3]
* if we want to concat along a specific dimension we have to spec the dimension index get it from shape) as second argument. so to concat along columns  `tensorA.concat(tensorB,1);` results in [[10, 20, 30, 70 , 80 , 90 ], [40, 50, 60, 100, 110, 120]] or shape [2,6]
* so default is 0
* the parameter is called axis of concatenation (Sounds like Python)

### Lecture 43 - Summing Values Along an Axis

* we showcase an example. we create 2 tensors
```
const jumpData = tf.tensor([
	[70,70,70],
  [70,70,70],	
	[70,70,70],	
	[70,70,70]
]);

const playerData = tf.tensor([
	[1,170],
  [2,170],	
	[3,170],	
	[4,170]
]);
```
* first has jumps for a player . one player per row
* we want to sum the jumps and concat the result on the player data tensor
* calling `jumpData.sum()` sums all elements up. we dont want that
* to sum along an axis we use `jumpData.sum(1)` as we want to sum along the column direction
* we get [210, 210, 210, 210] so our result is transformed to 1D
* we cannot directly concat to playerData. we need to reshape

### Lecture 44 - Massaging Dimensions with ExpandDims

* we need to trasform our sum to [[210],[210],[210],[210]] so from [4] to [4,1]
* sum results in Dimension Reduction!!!!
* concat needs identical dimensions
* to Avoid Dimension Reduction and keep original dimensions in a sum we use a second argument `jumpData.sum(1,true)` 
* now i can concat along the y axis (1) `jumpData.sum(1,true).concat(playerData,1)`
* another way which is tensorflow standard of ading dimensions is  using .expandDims() 
* expandDims accepts an axis on which the expand happens
* expandDims(0) expands the dimensions of the tensor by 1 on the x axis  so [4] => [1,4]
* expandDims(1) expands the dimensions of the tensor by 1 on the y axis  so [4] => [4,1]
* for our example `jumpData.sum(1).expandDims(1).concat(playerData,1)` solves our puzzle

## Section 4 - Applications of Tensorflow

### Lecture 45 - KNN with Regression

*  Steps to Follow:
	* Apply a slightly different KNN algorithm in the browser with Tensorflow JS and fake data
	* Move KNN algorithm to our code editor with real data and run in NodeJS environment
	* Do some optimization
* OUr next example will have to do with house prices
* we will have alist of properties with their location and price
* THe main difference from previous problem in the type of problem
	* drop ball. which bucket? => discrete label => classification
	* location + feats. price of house? linear label => regression
* Our current approach of KNN Algo
	* Find distance between features and prediction point
	* Sort from lowest point to greatest
	* Take the top K records
	* Average the label value of those top K records

### Lecture 46 - A Change in Data Structure

* our data will be fake dataset of 2 feats: longitute + latitude. the label will be a house price
* in the current approach we will split the dataset in features and labels (Python SKlearn style). so we will have 2 tensors
* the rational is that we will do tensorwide operations

### Lecture 47 - KNN with Tensorflow

* we test in JSPlaygrounds
* we set a fkae data set as tensors
```
const features = tf.tensor([
	[-121, 47],
  	[-121.2, 46.5],
	[-122, 46.4],
	[-120.9, 46.7]
]);


const labels = tf.tensor([
	[200],
    [250],
	[215],
	[240]
]);
```
* index of tensors matters as it maps feats to label
* we pass in a prediction point as tensor `const predictionPoint = tf.tensor([-121,46]);`
* we will write KNN in tensorflow.... using barebones linear algebra not bult in algos
* our KNN is 2D so we have 2D distance calc
* first we find distance from pred point (in 1d) using sub() and broadcasting `features.sub(predictionPoint)`
* the we need to square the diffs (square each eleemnt) using '.pow(2)' on the tensor (we use chaining)
* then we need to sum on the y axis. so we chain `.sum(1)`
* then we need to get the root 2  we chain `.pow(0.5)`

### Lecture 48 - Maintaining Order Relationships

* our next step is to sort the results (distances) from lower to greatest.
* shuffling the feats tensor breaks our index link to the labels tensor
* also tensors cannot be sorted
* to solve indexing we concat features wit labels
* first we need to solve dimensioning issue as duming reduces dimenstios
* we fix features dimensioning making it [4,1] with `	.expandDims(1)`
* then we concat along y axis labels `.concat(labels,1)`

### Lecture 49 - Sorting Tensors

* to sort we will use tensor method 'unstack()' which splits the tensor into an array of tensors along the specified axis
* then we can sort using JS
* chaining '.unstack()' splits the tensor into an array of tensor along the x axis (rows). so 1 tensor per row.
* we can access the each row tensor giving the row index `.unstack()[i]`
* all JS arrays come with the sort() method inbuilt
* e.g. 
```
const letters = ['b','a','d','c'];
letters.sort() // ['a', 'b', 'c' 'd']
```
* JS cannot sort tensors or even objects. No error. no result
* we need to pass in sort() a callback to tell it how to sort
* the callback gets 2 arguments say (a, b) these can be any array element do the comparizon and return 1 or -1 
	* 1 means a > b
	* -1 means b > a
* e.g
```
const distances = [
	{ value: 20},
	{ value: 30},
	{ value: 5},
	{ value: 10},
];

distances.sort((a,b) => {
	return a.value > b.value ? 1 : -1;
});
```
* for our problem we can access the 1st element of each tensor with its index and .get() so `.get(0)`.
* our sorting method that we chain becomes
```
	.sort((a,b)=>{
		return a.get(0) > b.get(0) ? 1 : -1;
	})
```

### Lecture 50 - Averaging Top Values

* to take the top k records we use '.slice()'
* note that after using unstack() in our chain we work with vanilla JS arrays so we use vanilla JS array slice().
* it uses a start point and the num of elements `.slice(0,k)`
* to get the average we sum vals together and use '.reduce()' to do it
* JS arrat reduce takes in a callback with 2 args. the  accumulator and the array element. after the callback it takes the accumulators init val. it iterates through the array moding the sum at our will
* to get the average val
```
.reduce((acc,tensor)=>{
  	return acc + tensor.get(1);
	},0)/k
```
* remember the hoyse val isa t index 1
* our tensorflow based custom KNN complete
```
features
  .sub(predictionPoint)
  .pow(2)
	.sum(1)
	.pow(.5)
	.expandDims(1)
	.concat(labels,1)
	.unstack()
	.sort((a,b)=>{
		return a.get(0) > b.get(0) ? 1 : -1;
	})
	.slice(0,k)
	.reduce((acc,tensor)=>{
  	return acc + tensor.get(1);
	},0)/k
```

### Lecture 51 - Moving to the Editor

* we go to /MLKits/knn-tf folder
* our code goes to index.js
* we also have a csv with real housing data + column titles 
* we also have a 'load_csv.js' a JS file to load the csv data
* 'pacakge.json' has all the libs in. we just have to 'npm install'

### Lecture 52 - Loading CSV Data

* we start writing our index.js file
* first we import tensorflow
```
require('@tensorflow/tfjs-node');
const tf = require('@tensorflow/tfjs');
```
* thje first import tells tf how to do the calculations. using the GPU or the CPU
* our import will try to do clacs on the cpu (for gpu use `require('@tensorflow/tfjs-node-gpu');`)
* the second import is our actual import  we can use in our program
* next we import the loadCSV js file `const loadCSV = require('./load-csv');`
* we call loadCSV to read the file. we pass in the csv file name and a config object that contains:
	* a suffle option (useful in ML)
	* a splitTest setting the record count for test
	* a 'dataColumns' to import in our dataset for our analysis
	* a 'labelColumns'
* we do destructuring to get the attributes of interest out of the generated object
```
let { features, labels, testFeatures, testLabels } = loadCSV('kc_house_data.csv', {
	shuffle: true,
	splitTest: 10,
	dataColumns: ['lat','long'],
	labelColumns: ['price']
});
```
* we cl to test code correctness
* we see that the tfjs build uses only generic CPU feats not harnessing our specific CPU feats that could boost performance
* we gan compile Tesorflow if we want on our CPU to boost performance

### Lecture 53 - Running an Analysis

* we cp all knn tf code from textbook in a tnn() function
* knn's signature is `function knn(features, labels, predictionPoint, k){}`
* we want to test the method. but all our sample data are plain arrays we have to turn them to tensors first
```
features = tf.tensor(features);
labels = tf.tensor(labels);
testFeatures = tf.tensor(testFeatures);
testLabels = tf.tensor(testLabels);
```
* we call our method for a testpoint and print our prediction
```
const result = knn(features,labels,testFeatures.slice([0,0],[1,-1]),10);
console.log(`Guess: ${result} Actual: ${testLabels.get(0,0)}`);
```
* our prediction is of as we take int oaccount only the location

### Lecture 54 - Reporting Error Percentages

* we will calucalte and report the error
* error = ((expected value) - (predicted value)) / (expected value)
* `error = (testLabels.get(0,0) - result) / testLabels.get(0,0);`
* we decide to work with testdata as arrays to iterate through testset printing the eror
```
features = tf.tensor(features);
labels = tf.tensor(labels);

const result = knn(features,labels,tf.tensor(testFeatures[0]),10);
const error = (testLabels[0][0]- result) / testLabels[0][0];
console.log(`Guess: ${result} Actual: ${testLabels[0][0]}`);
console.log(`Error: ${error * 100 }%`);
```
* we loop through the whole testSet with forEach()
```
testFeatures.forEach((testPoint, index)=>{
	const result = knn(features,labels,tf.tensor(testPoint),10);
	const error = (testLabels[index][0]- result) / testLabels[index][0];
	console.log(`Guess: ${result} Actual: ${testLabels[index][0]} Error: ${error * 100 }%`);
});
```
* we are almost consantly guesing below the actual

### Lecture 55 - Normalization or Standardization

* we need to include other feats, like size
* we mod our loadCsv cofig `dataColumns: ['lat','long', 'sqft_lot'],`
* knn is dimension agnostic so we rerun test. 
* our results improve but are not optimal... we will do normalization
* in visual code we can enable excel viewer to view csv data
* surface varies much mush more than the location so it has a much larger contibution to knn
* normalization or standarization? our surface vals have a normal distribution with some edge cases... no even distribution. so is a good candidate for standarization
* standarization is not affected by edge cases that spoil our metrics

### Lecture 56 - Numerical Standarization in Tensorflow

* the formula of standarization is: (Value - Average)/StandardDeviation
* standarization is applied per column
* we use 'tf.moments()' an inbuilt method passing the tesnor to calculate it on. this method returns an object which among other contains 'mean' AKA average and 'variance'.
* variance is very closely related sto stdDev as stdDev = sqrt(variance)
* tf.moments() works dimilarly to sum. it needs an axis. other wise it works on all datapoints
* an example of doing standarization on a sample array using tf
```
const numbers = tf.tensor([
	[1,2],
	[3,4],
	[5,6]
]);
const {mean, variance } = tf.moments(numbers,0)
numbers.sub(mean).div(variance.pow(.5))
```

### Lecture 57 - Applying Standarization

* we will add standarization in knn()
* we get the mean and varianve aof all used feats in knn `const {mean, variance } = tf.moments(features,0);`
* we apply standarization in predictionPoint `const scaledPrediction = predictionPoint.sub(mean).div(variance.pow(.5))`
* also we start our knn chain by standardizing feats
* also we need to pass our prediction point as tensor....
* our results are still off

### Lecture 58 - Debugging Calculations

* we will use node debugger and chrome debugging tools
* we run `node --inspect-brk index.js` and open chrome broweser
* we navigate to 'chrome://inspect'
* we see the remote process (index.js). we click inspect and see odevtools debugger
* we place a breakpoint in knn() and run code
* we console log features (shape and print) and look oc. we look at our scaled prediction
* it looks ok (close to 0)
* we cp code in console chaining .print() and look at iteration to spot abnormal vals

### Lecture 59 - What Now?

* to improve our algo we can do other steps like check different k vals . or ad dmore feats to the analyzis
* we add 'sqft_living' to feats. imrpovement is much better

## Section 5 - Getting Started with Gradient Descent

### Lecture 60 - Linear Regression

* Linear Regression Pros:
	* Its Fast. Only train once then use it for any prediction
	* Uses methods that will be very important in more complicated ML
* Linear Regression Cons:
	* Lot harder to understant with just intuition

### Lecture 61 - Why Linear Regression?

* with linear regression we try to find an equation that relates some inidmepte variable with a dependent variable
* a good way to understand it is linear fit. finding a line that passes among the datapoints and correlates 2 vars. the better correlated the vals the best the fit
* it works for multiple independent vars (features) 
* the equvalent is a multiple dimensional plot (abstract) where the fit is not a line but a n-1 dimensional plane


### Lecture 62 - Understanding Linear Regression

* There are many methods of solving linear regression problems
	* Ordinary Least Squares
	* Generalized Least Squares
	* ...
	* Gradient Descent

* we will use Gradient Descent. It is used in Deep Learning for imrpoving the results 
* We try tfind the mathematical relation between sqft lot size (x) and price (y)
* we do a first guess of y = 0x+1. we plot it and see that the distance of the datapoints and the line is big.
* for the computer to understand how bad its guess is it uses a way to calculate error . in our case Mean Squared Error (MSE) = 1/n Σi=1->n (Guess[i] - Actual[i])^2 where n is the number of datarows or samples
* The computer repeats running MSE for a new guess tr. then compares the value with previous one
* the fact is that a nd b in our equation will be the best possible for the lowest MSE

### Lecture 63 - Guessing Coefficients with MSE

* one approach is to focus on one coefficient at a time, plotting (MSE,b) on a chart.
* it will have an U shape where the lowest point is our best guess for the coefficient b
* this is not the best approach because:
	* we dont know the possible rqnge of b
	* we dont know a step size for incrementing b
	* huge computational demands when adding in more features 
* gradient descent works with derivatives and slope. (rememeber Bolzano theorem)

### Lecture 64 - Observations Around MSE

* we know that 'b' vs MSE chart always looks likea parabolic curve 'U' shaped
* when we get bad guesses the slope is steep. as we get better tjhe slope decreases
* 'The steepness of line tells us how far we are. the'
* 'The direction of the slope tell us if optimal b less or more the currect guess'
* SO what we care is the rte of change , the slope or derivative of MSE

### Lecture 65 - Derivatives!

* the derivative of an equation gives a new equation that tells us the slope or rate at any location
* for an 1 feat 1 label plot (2d) the MSA is a power 2 function. slope is a linear equation. we need just 2 points (guesses) to plot it and find where it crosses 0 so we can find with just 2 guesses where MSE is the least

### Lecture 66 - Gradient Descent in Action

* the derivative of MSE (assuming a coefficient of y=ax+b is 0) is:
	* MSE = 1/n Σi=1->n (Guess[i] - Actual[i])^2 
	* d(MSE)/db = 2/n Σi=1->n (b - Actual[i]) 
* Gradient Descent calculation (for only 1 coefficient):
	* Pick a value of 'b'
	* Calculate the slope of MSE with b
	* Is the slope very very small? if yes terminate
	* Multiple the slope by any arbitrary small value calleda 'learning rate'
	* Subtract that from 'b'
	* REPEAT from step 2
* Smaller learning rate => slower lagortihm => better results
* GDE slows down as we approach the best MSE when it finds it it stops

### Lecture 67 - Quick Breather and Review

* we want to find an equation that relates an independent variable and a dependent variable => House Price = a * (Sqft Lot) + b
* we can guess at bvalues of 'b', then use MSE to figure out how wrong we were
* the slope, or rate of change of MSE can be used to figure out whether our 'b' value was too high or low
* take a slope of MSE at 'b' using the derivative of MSE equation
* subtract the slope from 'b' to update the guess
* REPEAT from step 3

### Lecture 68 - Why a Learning Rate

* we will answere the following questions
	* why the learning rate
	* why use derivatives? why dont compute MSE twice and compare (brute force)
	* why not set derivative equal to 0 and solve for b to find minimum MSE
* learning rate controls the rate of convergence
* we adjust b based on slope and learning rate.
* in the initial slope we care about its sign. otherwise its pure intuition. we can use any number.
* vonvergence starts fast and slows as we go to the min. magnitude of initial slope is something we dont care at all. to point it out we start with the sign of iniital slope and replace it with a arbitrary number. it converges at the same rate and at the same point givent the same learning rate
* using learning rate and not fixed steps prevents overshoot.
* optimal learning rate is problem dependent
* plotting MSE vs b as we do the convergence shows the rate. (common in Python)

### Lecture 69 -  Answering Common Questions

* we use derivatives for GDE to save compute cycles
* solving for 0 (finding the vertex point) wont work when we enter multiple coefficients (like a and b) as we will have mulitple unknowns

### Lecture 70 - Gradient Descent with Multiple Terms

* we ll revice the GD algo to be able to find a and b that give minimum MSE.
* our reviced equations are:
	* MSE = 1/n Σi=1->n ((a * x[i] + b) - Actual[i])^2 
	* b partial MSE derivative d(MSE)/db = 2/n Σi=1->n ((a * x[i] + b) - Actual[i])
	* a partial MSE derivative d(MSE)/da = 2/n Σi=1->n ( -x[i](Actual - (a * x[i] + b))
* in essence we solve the lienar equation to a and b and use it in our MSE
* THe Mulitterm GD algorthm will be:
	* Pick a value for 'b' and 'a'
	* Calculate the slope of MSE with respect to 'a' and 'b'
	* Are both slopes small enough? if so quit
	* Multiply both slopes by learning rate
	* Subtract results from 'b' and 'a'
	* Repeat from step 2

### Lecture 71 - Multiple Terms in Action

* as a start point we choose a and b = 0 calculate slopes and iterate with learing rate of 0.01.
* this lr creates a big overshoot. we decrease
* we see that our nonscaled feats create a problem in convergence
* so when doing GD always to normalization or standarization before
* in real life lr is not a broblem as we can iterate as many times as appropriate

## Section 6 - Gradient Descent with Tensorflow

### Lecture 72 - Project Overview

* we go to /MLKits/regression and run `npm install`
* the files in the directory is a csv with its loader js code
* what we want is to find the relationship between milles per gallon (dependent feature) and horsepower (independent feature)
* 'Miles Per Gallon = a * (Car Horsepower) + b'
* What we will implement.
* A JS `class LinearRegression{}` with:
	* `gradientDescent()` : run one iteration of GD and update 'a' and 'b'
	* `train()` : Run GD until we get good values for 'a' and 'b'
	* `test()` : Use a 'test' data set to evaluate the accuracy o9f our calculated 'a' and 'b' 
	* predict() : Make a prediction using our calculated 'a' and 'b'

### Lecture 73 - Data Loading

* we add a file 'index.js' for app code and a 'linear-regression.js' for the algorithm code (reusable)
* we add tensorflow to index.js and the load-csv file
```
require('@tensorflow/tfjs-node');
const tf = require('@tensorflow/tfjs');
const loadCSV = require('./load-csv');
```
* we use loadCSV to get in array form the csv file columns of interest
```
let {features, labels, testFeatures, testLabels } =   loadCSV('./cars.csv',{
	shuffle: true,
	splitTest: 50,
	dataColumns: ['horsepower'],
	labelColumns: ['mpg']
});
```

### Lecture 74 - Default ALgorithm Options

* we add the 'linear-regression.js' file
* we import tf `const tf = require('@tensorflow/tfjs');`
* we define and export an empty LR class
```
class LinearRegression {

}

module.exports = LinearRegression;
```
* we add a constructor initializing the class instance `constructor(features, labels, options)` we assume that feats and labels are tensors and options a config object
* we add default options for the Algo
* we use Object.assign to copy an object to an empty object with some default params for the algo`this.options = Object.assign({ learningRate: 0.1, iterations: 1000 }, options);`
* Object.assign() is a good trick to have in mind
* iterations refers to max iterations (if not defined otherwise)

### Lecture 75 - Formulating the Training Loop

* we add a train() method to class for our training loop
```
	train() {
		for (let i = 0; i < this.options.iterations; i++) {
			this.gradientDescent()
		}
	}
```

### Lecture 76 - Initial Gradient Descent Implementation

* we will flesh out 'gradientDescent()'
* our first GD implementation will be working but slow (plain arrays)
* after we will replace it with a much faster more lean implem but more cryptic (fancy tf code)
* in constructor we set locals  a, b to 0
* in gradientDescent() we implement the slope equations.
* a * x + b is our guess so we will first calculate it for all data rows and then us it in the equation to make them easier. we use map to iterate throught the dataset and make a new array of guesses
```
		const currentGuessesForMPG = this.features.map(row => {
			return this.a * row[0] + this.b;
		});
```

### Lecture 77 - Calculating MSE Slopes

* we start by the b MSE derivative using map and lodash
```
		const bSlope = _.sum(currentGuessesForMPG.map((guess, i) => {
			return guess - this.labels[i][0];
		}))*2/this.labels.length;
```
* in same fashion we implement MSE derivative for a 
```
		const aSlope = _.sum(currentGuessesForMPG.map((guess, i) => {
			return -1*this.features[i][0]*(this.labels[i][0] -guess);
		}))*2/this.labels.length;
```

### Lecture 78 - Updating COefficients

* we need to use slopes to update the a and b vals after multiplyinmg them with learning rate
```
		this.a -= (aSlope * this.options.learningRate);
		this.b -= (bSlope * this.options.learningRate);
```
* we save and require in index the linearRegression class `const LinearRegression = require('./linear-regression');
`

### Lecture 79 - Interpreting Results

* we create a new instanc eof the class doing one run
```const regression = new LinearRegression(features, labels, { 
	learningRate: 0.001,
	iterations: 1
});
```
* we call `regression.train();` and in train we cl the a and b `console.log(`a: ${regression.a} b: ${regression.b}`);`
* we run 100 iterations values are way way off. our learning rate is off. we are overshooting we lower lr 0.0001 values are still off.
* now is the point where we should normalize our dataset to try to improve

### Lecture 80 - Matrix Multiplication

* matrices can be expressed as tensors
* Considerations on Matrix Multiplication:
	* Are matrices eligible to be multiplied together?
	* Whats the output of matrix multiplication?
	* how we multiply matrices?
* a matrix is a tensor with 2 dimensions
* inner shapes must be the same e.g [4,2] x [2,3] = [4,3]
* we cannot swap the order in matrix multiplication . the result is not the same

### Lecture 81 - More on Matrix Multiplication

* the shape of the output is the shape of the outer shapes of the two multipliers e.g [4,2] x [2,3] = [4,3]
* matrix multipkication is done row by column summing the multiplied elements
* the first column of the resulting matrix has elements that are multiplications of all rows of 1st multiplicant with 1 column of second
* [[1,2],[3,4]] x [[1,2],[3,4]] = [[1x1+2x3,3x1+4x3],[1x2+2x4,3x2+4x4]]

### Lecture 82 - Matrix form of Slope Equations

* we will take advantage of matrix multiplication to calculate the slope with respact to a and b. 
* the new equation is: (Features * ((Features * Weights) - Labels)) where
	* Labels: Tensor of label data
	* Features: Tensor of our feature data
	* n: Number of observations
	* Weights: A and B in a tensor

### Lecture 83 - Simplification with matrix Multiplication

* to get the guess with tensors and multiplications
	* we convert our indepenetent variables (features) to tensor and multiply it with a tensor that contains the weights
	* to use matric multiplication we need to make them compatible shape as now feats is [n,1] and weights [2,1] to solve it we expnd feats adding a column of 1 to the rights so that multiplication gives ax+b1 = ax+b
* the result will be [n,1] containing the guesses

### Lecture 84 - How it All Works Together