# Udemy Course: Machine Learning with Javascript by Stephen Grider

* [Course Link](https://www.udemy.com/machine-learning-with-javascript/)
* [Course Repo](https://github.com/StephenGrider/MLCasts)

## Section 1 - What is Machine Learning

### Lecture 2 - Solving Machine Learning Problems

* ML can do predicions based on historical data. e.g if it rain 240mm what the flood damage will there be
* The ML Problem Solving Process
	* Identify data that is relevant to the problem
	* Assemble a set of data related to the problem we're trying to solve
	* Decide on the type of output we are predicting
	* Based on type of output, pick an algorithm that will determine a correlations between our 'features' and 'labels'
	* Use model generated by algorithm to make a prediction
* For our example problem 'annual rainfall' is the feature or independent variable. Flood damage costs is the dependent variable 'label'
* in this course we will see how we can collect data format it and prepare it for ML
* we will assume we have no data. data can come from videos, newspapers, websites
* flood repair spending can come from budget reports. 
* all is assembled in a table

### Lecture 3 - A Complete Walkthrough

* Value of Labels are discrete set? Classification
* Value of Labes is continuous? Regression
* the example problem about floods is a regression problem (we predict money)
* Some takeaway points
	* Features are categories of data points that affect the value of labels
	* Datasets almost always need cleanup and formatting
	* Regression for continuous vals, Classification for diescrete
	* Many many different ML algorithms exist each with pros and cons anf or different type of problems.
	* models relate the value of features to the value of labels

### Lecture 4 - App Setup

* we go to [Github Starter Projects](https://github.com/StephenGrider/MLKits) and download them in our course workspace
* we go to '/MLKits/plinko'
* we open 'index.html'

### Lecture 5 - Problem Outline

* what we see is a game that puts (randomly?) disks in buckets increasing their counter.
* we will use it to build a dataset trying to predict in which bucket the ball will end to given the drop point

### Lecture 6 - Identifying Relevant Data

* apart from the priamry feature (drop position) we identify 2 minor features. ball size and bonciness. both are randonly selected between a configurable range of values

### Lecture 7 - Dataset Structures

* the app offers a lot of ways to record our tries and collect data (dataset)
* In JS datasets can be:
	* Arrays of objects eg `[{dropPosition: 300, bonciness: 0.4, ballSize: 16, bucket: 4}]`
	* Nested Arrays [[300,0,4,16,4],[350,0.4,35,5],[416,0.4,16,4]]. Indices are Important
* we open 'score.js'. it has 2 method signatures one to collect the data set and one to run the analyzsis to predict

### Lecture 8 - Recording Observation Data

* we flesh out 'onScoreUpdate' a callback  called everytime a round ends to collect data
```
const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition,bounciness,size,bucketLabel]);
  console.log(outputs);
}
```
* we test and see an evergroing nested array

### Lecture 9 - What Type of Problem?

* our problem is clearly a Classification
* our classification algorithm of choice is K-Nearest Neighbor (knn) is based on clustering

## Section 2 - Algorithm Overview

### Lecture 10 - How K-Nearest Neighbor Works

* Thought Experiment: what wqould happen if we dropped the ball 10 times from almost the same spot (300px)? 
* K-Nearest Neighbor (with one independent variable)
	* Drop a ball a bunch of times all around the board, record which bucket it goes to
	* For each observation subtract drop point from 300px, take absolute value
	* Sort ther results from least to greatest
	* Look at the 'k' top  records. What was the most common bucket?
	Whichever bucket came up most frequently is the one ours will probably go to

### Lecture 11 - Lodash Review

* we will use [lodash](https://lodash.com/docs/). a JS utility library with lots of methods for arrays, objects etc
* we use JS interpreter tool [JSPlaygrounds](https://stephengrider.github.io/JSPlaygrounds/) to experiment with code
* if we have a 4x2 nested array and we want to sort it by the row second element we can use stortBy selecting the element based on which we want to sort
```
const numbers = [[10,5],[17,2],[34,1],[60,-5]];
const sorted = _.sortBy(numbers, row => row[1])
```
* we map through the sorted array to extract second element `const mapped = _.map(sorted, row => row[1]);`
* we see a pattern forming where we use the previous result as input for next method .... function chaining maybe??? '_.chain' allows to chain lodash methods passing in result
```
_.chain(numbers)
	.sortBy(row=>row[1])
	.map(row=>row[1])
	.value();
```
* note that we omit the first argument as its implicitly passe don. vlaue() stops the chain adn returns the result

### Lecture 12 - Implementing KNN

* we start implementing KNN in JS using lodash in JSPlaygrounds
* we start by defining a hardcoded dataset as nested array
```
const outputs = [
	[10, .5, 16, 1].
  [200, .5, 16, 4],
  [350, .5, 16, 4],
  [600, .5, 16, 5]
];
```
* for e ach observation, we will subtract droppoint from 300px, and take the absolute value. we will use map. our result table will have to datapoints per observation. droppoint and bucket
* we put our distance calculation in a helper
```
const predictionPoint = 300;

const distance = (point) => {
	return Math.abs(point - predictionPoint);
};
```
* we implement step 1 
```

_.chain(outputs)
  .map(row => [distance(row[0]),row[3]])
```
* step 2 is to sort results from least to greatest. we use sortBy inthe chain `.sortBy(row => row[0])`
* step 3 is to look at top 'k' records. whats the most common bucket??? we use 'slice'. for k=3 `.slice(0,3)`

### Lecture 13 - Finishing KNN Implementation

* step 4 is to look in these k top records. whats the most common bucket? we will use lodash 'countBy' counting the records that meet some criteria. what we get is an object with key value pairs `.countBy(row => row[1])` => '{"1":1,"4":2}'
* next we will use lodash `.toPairs()` to convert the object to nested array
* we use sortBy to sort based on second column (occurencies). most will go to bottom
* i need t get the last element using lodash `.last()` and then `.first()` to get first element (bucket)
* then we use `.parseInt()` to turn string to bucket and `.value()` to terminate chain

### Lecture 14 - Testing the Algorithm

* we take the code and move it in the runAnalysis function in 'score.js'
* we cp distance function in the js file under analysis method, we cp globals on top and lodash chain in the runAnalysis method. we import lodash
* we test by running the game for multiple balls to fill our data set. then we 'analyze' to see in which bucket it will fall if we drop it from point 300. this will run our js knn method
* to test the result we 'reset' and drop 100 balls at 300
* we see our algorithm is way off

### Lecture 15 - Interpreting bad Results

* Steps after realising we have bad results:
	* Adjust the parameters of the analysis
	* Add more features to explain the analysis (bounciness, ball size)
	* Change the prediction point
	* Accept the fact there isn't a good correlation
* we will try out different k and rerun the analysis to see if it has effect on accuracy

### Lecture 16 - Test and Training Data

* to improve the algo we need to have a good way to compare accuracy with different settings. e.g for many different prediction points
* the way to find the ideal K
	* record a bunch of data points
	* split that data into a 'training' and a 'test' set
	* for each 'test' record run KNN using the 'training' data
	* Does the result of KNN equal the 'test' record bucket??

### Lecture 17 - Randomizing Test Data

* we implement a function to split the dataset in 2 groups
* we shuffle data to avoid bias `const shuffled = _.shuffle(data);`
* we split our dataset in 2 (train, test) using slice
```
	const testSet = _.slice(shuffled, 0, testCount);
	const trainingSet = _.slice(shuffled, testCount);
```
* we return them `return [testSet,trainingSet];`

### Lecture 18 - Generalizing KNN

* we will run 'runAnalysis()' many times for each row in testSet
* we put knn core in a helper function 'knn()' which will run on 'outputs' aka trainingSet multiple times
* in our recurcive runAnalysis each knn run will get a new predicition point which will be the dropPosition of the testSet row
* so we pass it as param passing it in the 'prediction()'
```
function knn(data, point) {
	 return _.chain(data)
	  	.map(row => [distance(row[0], point),row[3]])
		.sortBy(row => row[0])
		.slice(0,k)
		.countBy(row => row[1])
		.toPairs()
		.sortBy(row => row[1])
		.last()
		.first()
		.parseInt()
		.value();
}

function distance(pointA,pointB) {
	return Math.abs(pointA-pointB);
}
```

### Lecture 19 - Gauging Accuracy

* we assemble all pieces together in the runAnalysis function
* we split the dataset keeping 10 rows as testSet `const [testSet, trainingSet] = splitDataset(outputs, 10);`
* we do a for loop to run knn for each test datarow and we just console log result for now
```
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		console.log(bucket);
	}
```
* we test
* we need to compare knn predictions to the test set actual bucket results. we cl them  `console.log(bucket, testSet[i][3]);` accuracy is poor 

### Lecture 20 - Printing a Report

* we just increase a counter at each correct prediction and we do a cl in the end as report
```
function runAnalysis() {
	const testSetSize = 10;
	const [testSet, trainingSet] = splitDataset(outputs, testSetSize);

	let numberCorrect = 0;
	for (let i = 0; i < testSet.length; i++){
		const bucket = knn(trainingSet,testSet[i][0]);
		if (bucket === testSet[i][3]){
			numberCorrect++;
		}
```

### Lecture 21 - Refactoring Accuracy Reporting

* we refactor  runAnalysis using lodash and .chain()
* we use filter to reduce the array keeping only corect predictions
```
	const accuracy _.chain(testSet)
	 .filter(testPoint => knn(trainingSet, testPoint[0]) === testPoint[3])
	 .size()
	 .divide(testSetSize)
	 .value()
```

### Lecture 22 - Investigating Optimal K Values

* we will wrap ou runAnalisis testvcode in a for loop to test the results for different K vals
* we use lodash .range() instead of for loop)
* we also pass k as parameter at knn()
* we test but we dont see a trend in results
* we change test size to 50 and then to 100 also narrow or widen the k range
* we run k up to 20 and testsize of 100 we also drop balls 1 every pixel and analyze.. we fall in accuracy

### Lecture 23 - Updating KNN for Mutiple Features

* we go to step 2 of our imporvement attempt by adding more feats to the analysis
* we ll modify the algo for multiple variables
* the only change to our algo is that we now havbe to find distance in multiple dimensions (features)
* we will add 1 more feat (bounciness) so we will work in 2d space. distance will be ((x-x0)^2 + (y-y0)^2)^0.5

### Lecture 24 - Multi-Dimensional KNN

* in a 3d space the distance would be ((x-x0)^2 + (y-y0)^2 + (z-z0)^2)^0.5

### Lecture 25 - N-Dimension Distance

* we will use all 3 feats (droppoint, bounciness, ballsize)
* we need to mod the distance method. we will make it able to work for N -dimensions, not just 3
* distance will treat pointA and B as arrays of variable length
* we use lodash and .chain()
* we use .zip() join the 2 arrays as columns
* we use .map to square the diff of 2 nums using array destructutring
* ,sum() all 
* get the value() and square it to 0.5
```
function distance(pointA,pointB) {
	return _.chain(pointA)
			.zip(pointB)
			.map(([a,b])=> (a-b)**2)
			.sum()
			.value()**0.5;
}
```

### Lecture 26 - Arbitrary Feature Spaces

* we mod knn() to be able to pas sin dimension arrays of feats
* our label is always the last element. we use lodash .initial() to get n-1 first for feats and .last() to get last element
* we dont use .pop() from vanilla JS or shift as they mod the array
* we also use .initial() at point to get rid of label so that araays in distance match in length and zip() works
* this creates a problem in the future when we will want to use the model to do predictions (we wont have a label in the pointso it will fail)
```
	 return _.chain(data)
	  	.map(row => {
	  		return [distance(_.initial(row), point), _.last(row)];
	  	})
```
* so we should not use iniital(point) but manually clear out the array in the function call `.filter(testPoint => knn(trainingSet, _.initial(testPoint), k) === testPoint[3])`

### Lecture 27 - Magnitude Offsets in Features

* we retest for 10/10 dataset. now it takes a long time
* if we plot the points in real scale we see that there is no actual variation in bounciness. so we wont get a good indication from this feat. same for ball size as distances are squared
* this is solved with normalizing feats

### Lecture 28 - Feature Normaization

* we can normalize or standardize our data
* normaization: divide by max val so that tange is 0-1
* standarization: find the standard deviation and move the 0 of our range to this point
* what i get in standarization is a normal distribution around 0 (bell curve)
* for normalization we use MinMax method: Normalized DataSet = (FeatureVal - minOfFeatureVals)/(maxOfFeatureVals - minOfFeatureVals)
* we apply normization one feat at a time
* we test it in JSPlayground using lodash
* Normalization dramaticaly improves KNN

### Lecture 29 - Normalization with MinMax

* we add a new func passing in the data nad the num of feats aka columns we want to normalize `minMax(data, featureCount)`
* label should not get normalized
* we iterate through columns we extract them with .map()
* we get minn and max of column with lodash
* we iterating thriugh column applying minMax to each element
```

function minMax(data, featureCount) {
	const clonedData = _.cloneDeep(data);

	for(let i=0;i<featureCount;i++){
		const column = clonedData.map(row=>row[i]);
		const min = _.min(column);
		const max = _.max(column);
		for(let j=0;j<column.length;j++){
			clonedData[j][i] = (clonedData[j][i] -min) / (max -min);
		}
	}

	return clonedData;
}
```

### Lecture 30 - Applying Normalization

* we test it in console and apply it in runAnalysis() in the testrtrain split `const [testSet, trainingSet] = splitDataset(minMax(outputs, 3), testSetSize);`
* we test

### Lecture 31 - Feature Selection with KNN

* our results are bad even after normalizing
* ou intution says that:
	* Changes to Drop Position: Predictable changes to Output
	* Changes to Bounciness: Changes our output, but not predictably
* In Python plotting the correlation in a scatterplot would show all these
* We test the game playing with bounciness. we see that it has a detrimental effect to the analysis. we might get better ignoring it.
* Selecting features based on the corelation with label is Feature Selection
* if we dont have tools to prove the correlation (eg Python) we can run KNN for each feature and see the results (accuracy)

### Lecture 32 - Objective Feature Picking

* we mod runAnalysis()
* we will fix k and select a column (feat)
* we limit  range(0,3) so 0 1 2 to use it for column index. we will hardcode k
* we extract feature column from data and label `const data = _.map(outputs,row=> [row[feature], _.last(row)]);`
* we move tranitestsplit in analysis passing the new dataset `const [testSet, trainingSet] = splitDataset(minMax(data, 1), testSetSize);`
* we also make knn param for label parametrical `testPoint => knn(trainingSet, _.initial(testPoint), k) === _.last(testPoint)` 

### Lecture 33 - Evaluating Different Feature Values

* we see that indeed drop position has the largest effect in KNN
* the other 2 affect the result but cannot help us predict the result

## Section 3 - Onwards to Tensorflow JS!

### Lecture 34 - Let's Get our Bearings

* Key points from our very frugal into to ML:
	* Features vs Labels
	* Test vs Train sets of data
	* Feature Normalization
	* Common data structures (nested arrays)
	* Feature Selection
* Lodash:
* Pros: 
	* methods for just about everything we need
	* Excellent API desing (e.g. .chain())
	* Skills trasferable to other JS projects
* Cons: 
	* Extremely slow (relatively)
	* Not 'numbers' focused
	* Some things are awkward (getting columns of values)
* Tesorflow JS:
* Pros:
	* Similar API to Lodash
	Extremely Fast for numeric calcuilations
	* Has a 'low-level' linear algebra API + higher level API for ML
	* Similar API to numpy (popular Python numerical lib)
* Cons: 
	* still in active development

### Lecture 35 - A Plan to Move Forward

* Plan on tackling Tensorflow JS
	* Learn some fundamentals around Tensorflow JS
	* Go through a couple of exercises with Tensorflow
	* Rebuild KNN algorithm using Tensorflow
	* Build other algorithms with Tensorflow
* Rembember that:
	* The fastest way to learn ML is to master fundamental operations around working with Data
	* Strong knowledge of data handling basics makes applying any algorithm trivial

### Lecture 36 - Tensor Shape and Dimension

* [Tensorflow JS Site](https://js.tensorflow.org/)
* Tensorflow #1 Job when you begin ML is to make working with numbers in nested arrays really easy
* The core unit in a Tensorflow program is the Tensor (A JS object that wraps a collection of numbers structured in arrays)
* A Tensor in a program language agnostic definition is a multidimensional vector
* A core property of Tensors is Dimensions (like normal arrays in any language or tables in linear algebra)
* An easy way to tell the dimensions of a Tensor is to count the opening square braces
* Linear Algebra knowledge is a MUST
* Another core property of a Tensor is Shape: How many records (elements) in each dimension AKA size of array or Table (for JS remember .length on each dimension from outside in)
* e.g [[5,10,17],[18,4,2].length=3].length=2 => Shape [2,3]
* 2D is the most important dimension we will work with. for @D shape is [#rows, #columns]
* shapes are always in brackets. even for 1d

### Lecture 37  - Elementwise Operations

* 